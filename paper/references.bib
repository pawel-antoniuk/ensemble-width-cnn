%  introduction

@article{cherry_experiments_1953,
  title        = {Some experiments on the recognition of speech, with one and with two ears.},
  volume       = {25},
  issn         = {0001-4966(Print)},
  doi          = {10.1121/1.1907229},
  abstract     = {Experiments are described which examine the separation of two speech signals by human operators. 3 procedures were employed: presentation of both messages to both ears; presentation of one message to one ear with simultaneous presentation of the second message to the other ear; and presentation of a single message alternately to the two ears. Of particular current interest is the second procedure. Here, a listener is able to separate the messages exceedingly well despite the fact that he can only identify the general statistical properties of the rejected message. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
  pages        = {975--979},
  journaltitle = {Journal of the Acoustical Society of America},
  author       = {Cherry, E. Colin},
  date         = {1953},
  note         = {Place: {US}
                  Publisher: Acoustical Society of American}
}


@article{thiemann_speech_2016,
  title    = {Speech enhancement for multimicrophone binaural hearing aids aiming to preserve the spatial auditory scene},
  volume   = {2016},
  issn     = {1687-6180},
  url      = {https://doi.org/10.1186/s13634-016-0314-6},
  doi      = {10.1186/s13634-016-0314-6},
  abstract = {Modern binaural hearing aids utilize multimicrophone speech enhancement algorithms to enhance signals in terms of signal-to-noise ratio, but they may distort the interaural cues that allow the user to localize sources, in particular, suppressed interfering sources or background noise. In this paper, we present a novel algorithm that enhances the target signal while aiming to maintain the correct spatial rendering of both the target signal as well as the background noise. We use a bimodal approach, where a signal-to-noise ratio (SNR) estimator controls a binary decision mask, switching between the output signals of a binaural minimum variance distortionless response (MVDR) beamformer and scaled reference microphone signals. We show that the proposed selective binaural beamformer (SBB) can enhance the target signal while maintaining the overall spatial rendering of the acoustic scene.},
  number   = {1},
  urldate  = {2024-06-07},
  journal  = {EURASIP Journal on Advances in Signal Processing},
  author   = {Thiemann, Joachim and Müller, Menno and Marquardt, Daniel and Doclo, Simon and van de Par, Steven},
  month    = feb,
  year     = {2016},
  keywords = {Bilateral hearing aids, Binaural hearing aids, Binaural MVDR, Hearing aids},
  pages    = {12},
  file     = {Full Text PDF:/Users/pawel/Zotero/storage/HK2UGN3E/Thiemann et al. - 2016 - Speech enhancement for multimicrophone binaural he.pdf:application/pdf;Snapshot:/Users/pawel/Zotero/storage/MPJJ7SUT/s13634-016-0314-6.html:text/html}
}

@book{blauert_spatial_1996,
  title     = {Spatial {Hearing}: {The} {Psychophysics} of {Human} {Sound} {Localization}},
  isbn      = {978-0-262-26868-4},
  url       = {https://doi.org/10.7551/mitpress/6391.001.0001},
  abstract  = {The field of spatial hearing has exploded in the decade or so since Jens Blauert's classic work on acoustics was first published in English. This revised edition adds a new chapter that describes developments in such areas as auditory virtual reality (an important field of application that is based mainly on the physics of spatial hearing), binaural technology (modeling speech enhancement by binaural hearing), and spatial sound-field mapping. The chapter also includes recent research on the precedence effect that provides clear experimental evidence that cognition plays a significant role in spatial hearing. The remaining four chapters in this comprehensive reference cover auditory research procedures and psychometric methods, spatial hearing with one sound source, spatial hearing with multiple sound sources and in enclosed spaces, and progress and trends from 1972 (the first German edition) to 1983 (the first English edition)—work that includes research on the physics of the external ear, and the application of signal processing theory to modeling the spatial hearing process. There is an extensive bibliography of more than 900 items.},
  publisher = {The MIT Press},
  author    = {Blauert, Jens},
  month     = oct,
  year      = {1996},
  doi       = {10.7551/mitpress/6391.001.0001}
}


@article{zhang_surround_2017,
  title      = {Surround by {Sound}: {A} {Review} of {Spatial} {Audio} {Recording} and {Reproduction}},
  volume     = {7},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  issn       = {2076-3417},
  shorttitle = {Surround by {Sound}},
  url        = {https://www.mdpi.com/2076-3417/7/5/532},
  doi        = {10.3390/app7050532},
  abstract   = {In this article, a systematic overview of various recording and reproduction techniques for spatial audio is presented. While binaural recording and rendering is designed to resemble the human two-ear auditory system and reproduce sounds specifically for a listener’s two ears, soundfield recording and reproduction using a large number of microphones and loudspeakers replicate an acoustic scene within a region. These two fundamentally different types of techniques are discussed in the paper. A recent popular area, multi-zone reproduction, is also briefly reviewed in the paper. The paper is concluded with a discussion of the current state of the field and open problems.},
  language   = {en},
  number     = {5},
  urldate    = {2024-06-07},
  journal    = {Applied Sciences},
  author     = {Zhang, Wen and Samarasinghe, Parasanga N. and Chen, Hanchi and Abhayapala, Thushara D.},
  month      = may,
  year       = {2017},
  note       = {Number: 5
                Publisher: Multidisciplinary Digital Publishing Institute},
  keywords   = {binaural recording, binaural rendering, multi-zone reproduction, soundfield recording, soundfield reproduction, spatial audio},
  pages      = {532},
  file       = {Full Text PDF:/Users/pawel/Zotero/storage/8AJ5DLMH/Zhang et al. - 2017 - Surround by Sound A Review of Spatial Audio Recor.pdf:application/pdf}
}


@article{vera-diaz_towards_2018,
  title      = {Towards {End}-to-{End} {Acoustic} {Localization} {Using} {Deep} {Learning}: {From} {Audio} {Signals} to {Source} {Position} {Coordinates}},
  volume     = {18},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  issn       = {1424-8220},
  shorttitle = {Towards {End}-to-{End} {Acoustic} {Localization} {Using} {Deep} {Learning}},
  url        = {https://www.mdpi.com/1424-8220/18/10/3418},
  doi        = {10.3390/s18103418},
  abstract   = {This paper presents a novel approach for indoor acoustic source localization using microphone arrays, based on a Convolutional Neural Network (CNN). In the proposed solution, the CNN is designed to directly estimate the three-dimensional position of a single acoustic source using the raw audio signal as the input information and avoiding the use of hand-crafted audio features. Given the limited amount of available localization data, we propose, in this paper, a training strategy based on two steps. We first train our network using semi-synthetic data generated from close talk speech recordings. We simulate the time delays and distortion suffered in the signal that propagate from the source to the array of microphones. We then fine tune this network using a small amount of real data. Our experimental results, evaluated on a publicly available dataset recorded in a real room, show that this approach is able to produce networks that significantly improve existing localization methods based on SRP-PHAT strategies and also those presented in very recent proposals based on Convolutional Recurrent Neural Networks (CRNN). In addition, our experiments show that the performance of our CNN method does not show a relevant dependency on the speaker’s gender, nor on the size of the signal window being used.},
  language   = {en},
  number     = {10},
  urldate    = {2024-06-07},
  journal    = {Sensors},
  author     = {Vera-Diaz, Juan Manuel and Pizarro, Daniel and Macias-Guarasa, Javier},
  month      = oct,
  year       = {2018},
  note       = {Number: 10
                Publisher: Multidisciplinary Digital Publishing Institute},
  keywords   = {acoustic source localization, convolutional neural networks, deep learning, microphone arrays},
  pages      = {3418},
  file       = {Full Text PDF:/Users/pawel/Zotero/storage/QWRE2HZA/Vera-Diaz et al. - 2018 - Towards End-to-End Acoustic Localization Using Dee.pdf:application/pdf}
}


@inproceedings{yang_deepear_2022,
  title      = {{DeepEar}: {Sound} {Localization} with {Binaural} {Microphones}},
  shorttitle = {{DeepEar}},
  url        = {https://ieeexplore.ieee.org/document/9796850},
  doi        = {10.1109/INFOCOM48880.2022.9796850},
  abstract   = {Binaural microphones, referring to two microphones with artificial human-shaped ears, are pervasively used in humanoid robots and hearing aids improving sound quality. In many applications, it is crucial for such robots to interact with humans by finding the voice direction. However, sound source localization with binaural microphones remains challenging, especially in multi-source scenarios. Prior works utilize microphone arrays to deal with the multi-source localization problem. Extra arrays yet incur higher deployment costs and take up more space. However, human brains have evolved to locate multiple sound sources with only two ears. Inspired by this fact, we propose DeepEar, a binaural microphone-based localization system that can locate multiple sounds. To this end, we develop a neural network to mimic the acoustic signal processing pipeline of the human auditory system. Different from hand-crafted features used in prior works, DeepEar can automatically extract useful features for localization. More importantly, the trained neural networks can be extended and adapted to new environments with a minimum amount of extra training data. Experiment results show that DeepEar can substantially outperform the state-of-the-art deep learning approach, with a sound detection accuracy of 93.3\% and an azimuth estimation error of 7.4 degrees in multisource scenarios.},
  urldate    = {2024-06-07},
  booktitle  = {{IEEE} {INFOCOM} 2022 - {IEEE} {Conference} on {Computer} {Communications}},
  author     = {Yang, Qiang and Zheng, Yuanqing},
  month      = may,
  year       = {2022},
  note       = {ISSN: 2641-9874},
  keywords   = {Auditory system, Binaural localization, Deep learning, Ear, Earable computing, Feature extraction, Location awareness, Multi-source localization, Training data, Transfer learning},
  pages      = {960--969},
  file       = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/IN9CQI5D/9796850.html:text/html}
}

%  groups

@incollection{bregman_auditory_1990,
  title      = {Auditory {Scene} {Analysis}: {The} {Perceptual} {Organization} of {Sound}},
  volume     = {95},
  shorttitle = {Auditory {Scene} {Analysis}},
  abstract   = {Scitation is the online home of leading journals and conference proceedings from AIP Publishing and AIP Member Societies},
  booktitle  = {Journal of {The} {Acoustical} {Society} of {America} - {J} {ACOUST} {SOC} {AMER}},
  author     = {Bregman, Albert},
  month      = jan,
  year       = {1990},
  doi        = {10.1121/1.408434},
  note       = {Journal Abbreviation: Journal of The Acoustical Society of America - J ACOUST SOC AMER},
  file       = {Full Text PDF:/Users/pawel/Zotero/storage/SR53ZIS7/Bregman - 1990 - Auditory Scene Analysis The Perceptual Organizati.pdf:application/pdf},
  publisher  = {MIT Press}
}


@article{rumsey_spatial_2002,
  title        = {Spatial Quality Evaluation for Reproduced Sound: Terminology, Meaning, and a Scene-Based Paradigm},
  volume       = {50},
  shorttitle   = {Spatial Quality Evaluation for Reproduced Sound},
  abstract     = {Spatial quality in reproduced sound is a subset of the broad topic of sound quality. In the past it has been studied less rigorously than other aspects of reproduced sound quality, leading to a lack of clarity in standard definitions of subjective attributes. Rigor in the physical measurement of sound signals should be matched by equal rigor in semantics relating to subjective evaluation. A scene-based paradigm for the description and assessment of spatial quality is described, which enables clear distinctions to be made between elements of a reproduced sound scene and will assist in the search for related physical parameters.},
  pages        = {651--666},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {Journal of the Audio Engineering Society},
  author       = {Rumsey, Francis},
  date         = {2002-09-01}
}

@article{rumsey_spatial_2002,
  title        = {Spatial Quality Evaluation for Reproduced Sound: Terminology, Meaning, and a Scene-Based Paradigm},
  volume       = {50},
  shorttitle   = {Spatial Quality Evaluation for Reproduced Sound},
  abstract     = {Spatial quality in reproduced sound is a subset of the broad topic of sound quality. In the past it has been studied less rigorously than other aspects of reproduced sound quality, leading to a lack of clarity in standard definitions of subjective attributes. Rigor in the physical measurement of sound signals should be matched by equal rigor in semantics relating to subjective evaluation. A scene-based paradigm for the description and assessment of spatial quality is described, which enables clear distinctions to be made between elements of a reproduced sound scene and will assist in the search for related physical parameters.},
  pages        = {651--666},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {Journal of the Audio Engineering Society},
  author       = {Rumsey, Francis},
  date         = {2002-09-01}
}

%  related studies

%  multi-channel


@article{kaveh_statistical_1986,
  title        = {The statistical performance of the {MUSIC} and the minimum-norm algorithms in resolving plane waves in noise},
  volume       = {34},
  issn         = {0096-3518},
  url          = {https://ieeexplore.ieee.org/document/1164815},
  doi          = {10.1109/TASSP.1986.1164815},
  abstract     = {This paper presents an asymptotic statistical analysis of the null-spectra of two eigen-assisted methods, {MUSIC} [1] and Minimum-Norm [2], for resolving independent closely spaced plane waves in noise. Particular attention is paid to the average deviation of the null-spectra from zero at the true angles of arrival for the plane waves. These deviations are expressed as functions of signal-to-noise ratios, number of array elements, angular separation of emitters, and the number of snapshots. In the case of {MUSIC}. an approximate expression is derived for the resolution threshold of two plane waves with equal power in noise. This result is validated by Monte Carlo simulations.},
  pages        = {331--341},
  number       = {2},
  journaltitle = {{IEEE} Transactions on Acoustics, Speech, and Signal Processing},
  author       = {Kaveh, M. and Barabell, A.},
  urldate      = {2024-06-07},
  date         = {1986-04},
  note         = {Conference Name: {IEEE} Transactions on Acoustics, Speech, and Signal Processing},
  keywords     = {Covariance matrix, Direction of arrival estimation, Monte Carlo methods, Multiple signal classification, Narrowband, Signal resolution, Signal to noise ratio, Spatial resolution, Spectral analysis, Statistical analysis}
}




@inproceedings{pavlidi_real-time_2012,
  title      = {Real-time multiple sound source localization using a circular microphone array based on single-source confidence measures},
  url        = {https://ieeexplore.ieee.org/document/6288455},
  doi        = {10.1109/ICASSP.2012.6288455},
  abstract   = {We propose a novel real-time adaptative localization approach for multiple sources using a circular array, in order to suppress the localization ambiguities faced with linear arrays, and assuming a weak sound source sparsity which is derived from blind source separation methods. Our proposed method performs very well both in simulations and in real conditions at 50\% real-time.},
  eventtitle = {2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  pages      = {2625--2628},
  booktitle  = {2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  author     = {Pavlidi, Despoina and Puigt, Matthieu and Griffin, Anthony and Mouchtaris, Athanasios},
  urldate    = {2024-06-07},
  date       = {2012-03},
  note       = {{ISSN}: 2379-190X},
  keywords   = {Array signal processing, Arrays, direction of arrival estimation, Direction of arrival estimation, Estimation, Microphones, multiple source localization, Real time systems, Speech, Time frequency analysis},
  file       = {Full Text:/Users/pawel/Zotero/storage/FXAAV3EW/Pavlidi et al. - 2012 - Real-time multiple sound source localization using.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/RI3ZVKSY/6288455.html:text/html}
}



@article{pan_multi-tone_2021,
  title        = {Multi-Tone Phase Coding of Interaural Time Difference for Sound Source Localization With Spiking Neural Networks},
  volume       = {29},
  issn         = {2329-9304},
  url          = {https://ieeexplore.ieee.org/document/9502013},
  doi          = {10.1109/TASLP.2021.3100684},
  abstract     = {Mammals exhibit remarkable capability of detecting and localizing sound sources in complex acoustic environments by using binaural cues in the spiking manner. Emulating the auditory process for sound source localization ({SSL}) by mammals, we propose a computational model for accurate and robust {SSL} under the neuromorphic spiking neural network ({SNN}) framework. The center of this model is a Multi-Tone Phase Coding ({MTPC}) scheme, which encodes the interaural time difference ({ITD}) between binaural pure tones into discriminative spike patterns that can be directly classified by {SNNs}. As such, {SSL} can be implemented as an event-driven task on highly efficient, neuromorphic parallel processors. We evaluate the proposed computational model on a directional audio dataset recorded from a microphone array in a realistic acoustic environment with background noise, obstruction, reflection, and other interferences. We report superior localization capability with a mean absolute error ({MAE}) of 1.02° or 100\% classification accuracy with an angle resolution of 5°, which surpasses other {SNN}-based biologically plausible neuromorphic approaches by a relatively large margin and on par with human performance in similar tasks. This study opens up many application opportunities in human-robot interaction where energy efficiency is crucial. As a case study, we successfully deploy the proposed {SSL} system in a robotic platform to track the speaker and orient the robot's attention.},
  pages        = {2656--2670},
  journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  author       = {Pan, Zihan and Zhang, Malu and Wu, Jibin and Wang, Jiadong and Li, Haizhou},
  urldate      = {2024-06-07},
  date         = {2021},
  note         = {Conference Name: {IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  keywords     = {Acoustics, Biological neural networks, Computational modeling, Ear, Encoding, Location awareness, Neural phase coding, Neurons, sound source localization, spiking neural network},
  file         = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/8CJB3YFJ/9502013.html:text/html}
}



@article{hahmann_sound_2022,
  title        = {Sound source localization using multiple ad hoc distributed microphone arrays},
  volume       = {2},
  issn         = {2691-1191},
  doi          = {10.1121/10.0011811},
  abstract     = {Sound source localization is crucial for communication and sound scene analysis. This study uses direction-of-arrival estimates of multiple ad hoc distributed microphone arrays to localize sound sources in a room. An affine mapping between the independent array estimates and the source coordinates is derived from a set of calibration points. Experiments show that the affine model is sufficient to locate a source and can be calibrated to physical dimensions. A projection of the local array estimates increases localization accuracy, particularly further away from the calibrated region. Localization tests in three dimensions compare the affine approach to a nonlinear neural network.},
  pages        = {074801},
  number       = {7},
  journaltitle = {{JASA} express letters},
  shortjournal = {{JASA} Express Lett},
  author       = {Hahmann, Manuel and Fernandez-Grande, Efren and Gunawan, Henrry and Gerstoft, Peter},
  date         = {2022-07},
  pmid         = {36154052},
  keywords     = {Acoustics, Sound, Sound Localization},
  file         = {Full Text:/Users/pawel/Zotero/storage/5MD884GP/Hahmann et al. - 2022 - Sound source localization using multiple ad hoc di.pdf:application/pdf}
}



@article{chung_sound_2022,
  title        = {Sound Localization Based on Acoustic Source Using Multiple Microphone Array in an Indoor Environment},
  volume       = {11},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2079-9292},
  url          = {https://www.mdpi.com/2079-9292/11/6/890},
  doi          = {10.3390/electronics11060890},
  abstract     = {Sound signals have been widely applied in various fields. One of the popular applications is sound localization, where the location and direction of a sound source are determined by analyzing the sound signal. In this study, two microphone linear arrays were used to locate the sound source in an indoor environment. The {TDOA} is also designed to deal with the problem of delay in the reception of sound signals from two microphone arrays by using the generalized cross-correlation algorithm to calculate the {TDOA}. The proposed microphone array system with the algorithm can successfully estimate the sound source’s location. The test was performed in a standardized chamber. This experiment used two microphone arrays, each with two microphones. The experimental results prove that the proposed method can detect the sound source and obtain good performance with a position error of about 2.0{\textasciitilde}2.3 cm and angle error of about 0.74 degrees. Therefore, the experimental results demonstrate the feasibility of the system.},
  pages        = {890},
  number       = {6},
  journaltitle = {Electronics},
  author       = {Chung, Ming-An and Chou, Hung-Chi and Lin, Chia-Wei},
  urldate      = {2024-06-07},
  date         = {2022-01},
  langid       = {english},
  note         = {Number: 6
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {generalized cross-correlation algorithm, indoor localization, microphone array, sound localization, time difference of arrival},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/PX2QWICQ/Chung et al. - 2022 - Sound Localization Based on Acoustic Source Using .pdf:application/pdf}
}



@article{liu_sound_2022,
  title        = {Sound Source Localization Based on Multi-Channel Cross-Correlation Weighted Beamforming},
  volume       = {13},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2072-666X},
  url          = {https://www.mdpi.com/2072-666X/13/7/1010},
  doi          = {10.3390/mi13071010},
  abstract     = {Beamforming and its applications in steered-response power ({SRP}) technology, such as steered-response power delay and sum ({SRP}-{DAS}) and steered-response power phase transform ({SRP}-{PHAT}), are widely used in sound source localization. However, their resolution and accuracy still need improvement. A novel beamforming method combining {SRP} and multi-channel cross-correlation coefficient ({MCCC}), {SRP}-{MCCC}, is proposed in this paper to improve the accuracy of direction of arrival ({DOA}). Directional weight ({DW}) is obtained by calculating the {MCCC}. Based on {DW}, suppressed the non-incoming wave direction and gained the incoming wave direction to improve the beamforming capabilities. Then, sound source localizations based on the dual linear array under different conditions were simulated. Compared with {SRP}-{PHAT}, {SRP}-{MCCC} has the advantages of high positioning accuracy, strong spatial directivity and robustness under the different signal–noise ratios ({SNRs}). When the {SNR} is −10 {dB}, the average positioning error of the single-frequency sound source at different coordinates decreases by 5.69\%, and that of the mixed frequency sound sources at the same coordinate decreases by 5.77\%. Finally, the experimental verification was carried out. The results show that the average error of {SRP}-{MCCC} has been reduced by 8.14\% and the positioning accuracy has been significantly improved, which is consistent with the simulation results. This research provides a new idea for further engineering applications of sound source localization based on beamforming.},
  pages        = {1010},
  number       = {7},
  journaltitle = {Micromachines},
  author       = {Liu, Mengran and Hu, Junhao and Zeng, Qiang and Jian, Zeming and Nie, Lei},
  urldate      = {2024-06-07},
  date         = {2022-07},
  langid       = {english},
  note         = {Number: 7
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {beamforming, microphone array, multi-channel cross-correlation coefficient, sound source localization},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/J597NXKN/Liu et al. - 2022 - Sound Source Localization Based on Multi-Channel C.pdf:application/pdf}
}



%  individual sources


@article{dietz_auditory_2011,
  series   = {Perceptual and {Statistical} {Audition}},
  title    = {Auditory model based direction estimation of concurrent speakers from binaural signals},
  volume   = {53},
  issn     = {0167-6393},
  url      = {https://www.sciencedirect.com/science/article/pii/S016763931000097X},
  doi      = {10.1016/j.specom.2010.05.006},
  abstract = {Humans show a very robust ability to localize sounds in adverse conditions. Computational models of binaural sound localization and technical approaches of direction-of-arrival (DOA) estimation also show good performance, however, both their binaural feature extraction and the strategies for further analysis partly differ from what is currently known about the human auditory system. This study investigates auditory model based DOA estimation emphasizing known features and limitations of the auditory binaural processing such as (i) high temporal resolution, (ii) restricted frequency range to exploit temporal fine-structure, (iii) use of temporal envelope disparities, and (iv) a limited range to compensate for interaural time delay. DOA estimation performance was investigated for up to five concurrent speakers in free field and for up to three speakers in the presence of noise. The DOA errors in these conditions were always smaller than 5°. A condition with moving speakers was also tested and up to three moving speakers could be tracked simultaneously. Analysis of DOA performance as a function of the binaural temporal resolution showed that short time constants of about 5ms employed by the auditory model were crucial for robustness against concurrent sources.},
  number   = {5},
  urldate  = {2024-06-07},
  journal  = {Speech Communication},
  author   = {Dietz, Mathias and Ewert, Stephan D. and Hohmann, Volker},
  month    = may,
  year     = {2011},
  keywords = {Auditory modeling, Binaural processing, Direction estimation},
  pages    = {592--605},
  file     = {ScienceDirect Snapshot:/Users/pawel/Zotero/storage/B6MXZWGK/S016763931000097X.html:text/html}
}


@article{may_probabilistic_2011,
  title    = {A {Probabilistic} {Model} for {Robust} {Localization} {Based} on a {Binaural} {Auditory} {Front}-{End}},
  volume   = {19},
  issn     = {1558-7924},
  url      = {https://ieeexplore.ieee.org/document/5406118},
  doi      = {10.1109/TASL.2010.2042128},
  abstract = {Although extensive research has been done in the field of machine-based localization, the degrading effect of reverberation and the presence of multiple sources on localization performance has remained a major problem. Motivated by the ability of the human auditory system to robustly analyze complex acoustic scenes, the associated peripheral stage is used in this paper as a front-end to estimate the azimuth of sound sources based on binaural signals. One classical approach to localize an acoustic source in the horizontal plane is to estimate the interaural time difference (ITD) between both ears by searching for the maximum in the cross-correlation function. Apart from ITDs, the interaural level difference (ILD) can contribute to localization, especially at higher frequencies where the wavelength becomes smaller than the diameter of the head, leading to ambiguous ITD information. The interdependency of ITD and ILD on azimuth is a complex pattern that depends also on the room acoustics, and is therefore learned by azimuth-dependent Gaussian mixture models (GMMs). Multiconditional training is performed to take into account the variability of the binaural features which results from multiple sources and the effect of reverberation. The proposed localization model outperforms state-of-the-art localization techniques in simulated adverse acoustic conditions.},
  number   = {1},
  urldate  = {2024-06-07},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {May, Tobias and van de Par, Steven and Kohlrausch, Armin},
  month    = jan,
  year     = {2011},
  note     = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {auditory scene analysis (ASA), Auditory system, Azimuth, binaural, Degradation, Ear, Frequency, Humans, interaural level difference (ILD), interaural time difference (ITD), Layout, Localization, reverberation, Reverberation, Robustness, Signal analysis},
  pages    = {1--13}
}


@article{may_binaural_2012,
  title    = {A {Binaural} {Scene} {Analyzer} for {Joint} {Localization} and {Recognition} of {Speakers} in the {Presence} of {Interfering} {Noise} {Sources} and {Reverberation}},
  volume   = {20},
  issn     = {1558-7924},
  url      = {https://ieeexplore.ieee.org/document/6178270},
  doi      = {10.1109/TASL.2012.2193391},
  abstract = {In this study, we present a binaural scene analyzer that is able to simultaneously localize, detect and identify a known number of target speakers in the presence of spatially positioned noise sources and reverberation. In contrast to many other binaural cocktail party processors, the proposed system does not require a priori knowledge about the azimuth position of the target speakers. The proposed system consists of three main building blocks: binaural localization, speech source detection, and automatic speaker identification. First, a binaural front-end is used to robustly localize relevant sound source activity. Second, a speech detection module based on missing data classification is employed to determine whether detected sound source activity corresponds to a speaker or to an interfering noise source using a binary mask that is based on spatial evidence supplied by the binaural front-end. Third, a second missing data classifier is used to recognize the speaker identities of all detected speech sources. The proposed system is systematically evaluated in simulated adverse acoustic scenarios. Compared to state-of-the art MFCC recognizers, the proposed model achieves significant speaker recognition accuracy improvements.},
  number   = {7},
  urldate  = {2024-06-07},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {May, Tobias and van de Par, Steven and Kohlrausch, Armin},
  month    = sep,
  year     = {2012},
  note     = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {Acoustics, Auditory system, Automatic speaker recognition, binaural processing, computational auditory scene analysis (CASA), Humans, mask estimation, missing data, Noise, Speech, Speech recognition, Target recognition},
  pages    = {2016--2030}
}


@article{woodruff_binaural_2012,
  title    = {Binaural {Localization} of {Multiple} {Sources} in {Reverberant} and {Noisy} {Environments}},
  volume   = {20},
  issn     = {1558-7924},
  url      = {https://ieeexplore.ieee.org/document/6129395},
  doi      = {10.1109/TASL.2012.2183869},
  abstract = {Sound source localization from a binaural input is a challenging problem, particularly when multiple sources are active simultaneously and reverberation or background noise are present. In this work, we investigate a multi-source localization framework in which monaural source segregation is used as a mechanism to increase the robustness of azimuth estimates from a binaural input. We demonstrate performance improvement relative to binaural only methods assuming a known number of spatially stationary sources. We also propose a flexible azimuth-dependent model of binaural features that independently captures characteristics of the binaural setup and environmental conditions, allowing for adaptation to new environments or calibration to an unseen binaural setup. Results with both simulated and recorded impulse responses show that robust performance can be achieved with limited prior training.},
  number   = {5},
  urldate  = {2024-06-07},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Woodruff, John and Wang, DeLiang},
  month    = jul,
  year     = {2012},
  note     = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {Adaptation models, Azimuth, Binaural sound localization, computational auditory scene analysis (CASA), Estimation, Feature extraction, monaural grouping, Noise measurement, reverberation, Reverberation, Time frequency analysis},
  pages    = {1503--1512},
  file     = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/9CW9FT7X/6129395.html:text/html}
}


@inproceedings{may_robust_2015,
  title     = {Robust localisation of multiple speakers exploiting head movements and multi-conditional training of binaural cues},
  url       = {https://ieeexplore.ieee.org/document/7178457},
  doi       = {10.1109/ICASSP.2015.7178457},
  abstract  = {This paper addresses the problem of localising multiple competing speakers in the presence of room reverberation, where sound sources can be positioned at any azimuth on the horizontal plane. To reduce the amount of front-back confusions which can occur due to the similarity of interaural time differences (ITDs) and interaural level differences (ILDs) in the front and rear hemifield, a machine hearing system is presented which combines supervised learning of binaural cues using multi-conditional training (MCT) with a head movement strategy. A systematic evaluation showed that this approach substantially reduced the amount of front-back confusions in challenging acoustic scenarios. Moreover, the system was able to generalise to a variety of different acoustic conditions not seen during training.},
  urldate   = {2024-06-07},
  booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author    = {May, Tobias and Ma, Ning and Brown, Guy J.},
  month     = apr,
  year      = {2015},
  note      = {ISSN: 2379-190X},
  keywords  = {Acoustics, Auditory system, Azimuth, binaural sound source localisation, generalisation, head movements, Magnetic heads, multi-conditional training, Robustness, Speech, Training},
  pages     = {2679--2683},
  file      = {Accepted Version:/Users/pawel/Zotero/storage/UUF7ABAN/May et al. - 2015 - Robust localisation of multiple speakers exploitin.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/VAZXWS4I/7178457.html:text/html}
}

@inproceedings{ma16c_interspeech,
  author    = {Ning Ma and Guy J. Brown},
  title     = {{Speech Localisation in a Multitalker Mixture by Humans and Machines}},
  year      = 2016,
  booktitle = {Proc. Interspeech 2016},
  pages     = {3359--3363},
  doi       = {10.21437/Interspeech.2016-1149},
  issn      = {2308-457X}
}

@article{benaroya_binaural_2018,
  title    = {Binaural {Localization} of {Multiple} {Sound} {Sources} by {Non}-{Negative} {Tensor} {Factorization}},
  volume   = {26},
  issn     = {2329-9304},
  url      = {https://ieeexplore.ieee.org/document/8294267},
  doi      = {10.1109/TASLP.2018.2806745},
  abstract = {This paper presents non-negative factorization of audio signals for the binaural localization of multiple sound sources within realistic and unknown sound environments. Non-negative tensor factorization (NTF) provides a sparse representation of multichannel audio signals in time, frequency, and space that can be exploited in computational audio scene analysis and robot audition for the separation and localization of sound sources. In the proposed formulation, each sound source is represented by means of spectral dictionaries, temporal activation, and its distribution within each channel (here, left and right ears). This distribution, being dependent on the frequency, can be interpreted as an explicit estimation of the Head-Related Transfer Function (HRTF) of a binaural head which can then be converted into the estimated sound source position. Moreover, the semisupervised formulation of the non-negative factorization allows us to integrate prior knowledge about some sound sources of interest whose dictionaries can be learned in advance, whereas the remaining sources are considered as background sound, which remains unknown and is estimated on the fly. The proposed NTF-based sound source localization is applied here to binaural sound source localization of multiple speakers within realistic sound environments.},
  number   = {6},
  urldate  = {2024-06-07},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Benaroya, Elie Laurent and Obin, Nicolas and Liuni, Marco and Roebel, Axel and Raumel, Wilson and Argentieri, Sylvain},
  month    = jun,
  year     = {2018},
  note     = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  keywords = {Binaural localization, computational audio scene analysis, Ear, Image analysis, non-negative tensor factorization, robot audition, Robot kinematics, Speech, Speech processing, Tensile stress},
  pages    = {1072--1082},
  file     = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/AGTH4VGH/8294267.html:text/html;Submitted Version:/Users/pawel/Zotero/storage/TERC7QZT/Benaroya et al. - 2018 - Binaural Localization of Multiple Sound Sources by.pdf:application/pdf}
}


%  assumed number of sources

@article{vecchiotti19,
  author     = {Paolo Vecchiotti and
                Ning Ma and
                Stefano Squartini and
                Guy J. Brown},
  title      = {End-to-end Binaural Sound Localisation from the Raw Waveform},
  journal    = {CoRR},
  volume     = {abs/1904.01916},
  year       = {2019},
  url        = {http://arxiv.org/abs/1904.01916},
  eprinttype = {arXiv},
  eprint     = {1904.01916},
  timestamp  = {Sat, 30 Sep 2023 10:08:12 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1904-01916.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{ma_exploiting_2017,
  title    = {Exploiting {Deep} {Neural} {Networks} and {Head} {Movements} for {Robust} {Binaural} {Localisation} of {Multiple} {Sources} in {Reverberant} {Environments}},
  volume   = {25},
  issn     = {2329-9290, 2329-9304},
  url      = {http://arxiv.org/abs/1904.03001},
  doi      = {10.1109/TASLP.2017.2750760},
  abstract = {This paper presents a novel machine-hearing system that exploits deep neural networks (DNNs) and head movements for robust binaural localisation of multiple sources in reverberant environments. DNNs are used to learn the relationship between the source azimuth and binaural cues, consisting of the complete cross-correlation function (CCF) and interaural level differences (ILDs). In contrast to many previous binaural hearing systems, the proposed approach is not restricted to localisation of sound sources in the frontal hemifield. Due to the similarity of binaural cues in the frontal and rear hemifields, front-back confusions often occur. To address this, a head movement strategy is incorporated in the localisation model to help reduce the front-back errors. The proposed DNN system is compared to a Gaussian mixture model (GMM) based system that employs interaural time differences (ITDs) and ILDs as localisation features. Our experiments show that the DNN is able to exploit information in the CCF that is not available in the ITD cue, which together with head movements substantially improves localisation accuracies under challenging acoustic scenarios in which multiple talkers and room reverberation are present.},
  number   = {12},
  urldate  = {2024-06-07},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Ma, Ning and May, Tobias and Brown, Guy J.},
  month    = dec,
  year     = {2017},
  note     = {arXiv:1904.03001 [cs, eess]},
  keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  pages    = {2444--2453},
  annote   = {Comment: 10 pages},
  file     = {arXiv Fulltext PDF:/Users/pawel/Zotero/storage/Q8KYT7PT/Ma et al. - 2017 - Exploiting Deep Neural Networks and Head Movements.pdf:application/pdf;arXiv.org Snapshot:/Users/pawel/Zotero/storage/NXKLQ8NH/1904.html:text/html}
}

@article{pang_multitask_2019,
  title    = {Multitask {Learning} of {Time}-{Frequency} {CNN} for {Sound} {Source} {Localization}},
  volume   = {7},
  issn     = {2169-3536},
  url      = {https://ieeexplore.ieee.org/document/8668414},
  doi      = {10.1109/ACCESS.2019.2905617},
  abstract = {Sound source localization (SSL) is an important technique for many audio processing systems, such as speech enhancement/recognition and human-robot interaction. Although many methods have been proposed for SSL, it still remains a challenging task to achieve accurate localization under adverse acoustic scenarios. In this paper, a novel binaural SSL method based on time-frequency convolutional neural network (TF-CNN) with multitask learning is proposed to simultaneously localize azimuth and elevation under unknown acoustic conditions. First, the interaural phase difference and interaural level difference are extracted from the received binaural signals, which are taken as the input of the proposed SSL neural network. Then, an SSL neural network is designed to map the interaural cues to sound direction, which consists of TF-CNN module and multitask neural network. The TF-CNN module learns and combines the time-frequency information of extracted interaural cues to generate the shared feature for multitask SSL. With the shared feature, a multitask neural network is designed to simultaneously estimate azimuth and elevation through multitask learning, which generates the posterior probability for candidate directions. Finally, the candidate direction with the highest probability is taken as the final direction estimation. The experiments based on public head-related transfer function (HRTF) database demonstrate that the proposed method achieves preferable localization performance compared with other popular methods.},
  urldate  = {2024-06-07},
  journal  = {IEEE Access},
  author   = {Pang, Cheng and Liu, Hong and Li, Xiaofei},
  year     = {2019},
  note     = {Conference Name: IEEE Access},
  keywords = {Acoustics, Azimuth, convolutional neural network, Estimation, Feature extraction, multitask learning, Neural networks, Noise measurement, Sound source localization, time-frequency, Time-frequency analysis},
  pages    = {40725--40737}
}


@misc{s_spatiogram_2021,
  title      = {Spatiogram: {A} phase based directional angular measure and perceptual weighting for ensemble source width},
  shorttitle = {Spatiogram},
  url        = {http://arxiv.org/abs/2112.07216},
  doi        = {10.48550/arXiv.2112.07216},
  abstract   = {In concert hall studies, inter-aural cross-correlation (IACC), which is signal dependent, is used as a measure of perceptual source width. The same measure is used for perceptual source width in the case of distributed sources also. In this work, we examine the validity of IACC for both the cases and develop an improved measure for ensemble-like distributed sources. We decompose the new objective measure for perceptual ensemble source width (ESW) into two components (i) phase based directional angular measure, which is timbre independent (spatial measure) and (ii) mean time-bandwidth energy (MTBE), a perceptual weight, (timbre measure). This combination of spatial and timbral measures can be extended as an alternate measure for determining auditory source width (ASW) and listener envelopment (LEV) of arbitrary signals in concert-hall and room acoustics.},
  urldate    = {2024-06-07},
  publisher  = {arXiv},
  author     = {S, Arthi and T V, Sreenivas},
  month      = dec,
  year       = {2021},
  note       = {arXiv:2112.07216 [cs, eess]},
  keywords   = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  annote     = {Comment: 12 pages, 11 figures},
  file       = {arXiv Fulltext PDF:/Users/pawel/Zotero/storage/IRU2Y2LJ/S and T V - 2021 - Spatiogram A phase based directional angular meas.pdf:application/pdf;arXiv.org Snapshot:/Users/pawel/Zotero/storage/TXPUMCRK/2112.html:text/html}
}

# speech


@article{wang_binaural_2020,
  title        = {Binaural sound localization based on deep neural network and affinity propagation clustering in mismatched {HRTF} condition},
  volume       = {2020},
  issn         = {1687-4722},
  url          = {https://doi.org/10.1186/s13636-020-0171-y},
  doi          = {10.1186/s13636-020-0171-y},
  abstract     = {Binaural sound source localization is an important and widely used perceptually based method and it has been applied to machine learning studies by many researchers based on head-related transfer function ({HRTF}). Because the {HRTF} is closely related to human physiological structure, the {HRTFs} vary between individuals. Related machine learning studies to date tend to focus on binaural localization in reverberant or noisy environments, or in conditions with multiple simultaneously active sound sources. In contrast, mismatched {HRTF} condition, in which the {HRTFs} used to generate the training and test sets are different, is rarely studied. This mismatch leads to a degradation of localization performance. A basic solution to this problem is to introduce more data to improve generalization performance, which requires a lot. However, simply increasing the data volume will result in data-inefficiency. In this paper, we propose a data-efficient method based on deep neural network ({DNN}) and clustering to improve binaural localization performance in the mismatched {HRTF} condition. Firstly, we analyze the relationship between binaural cues and the sound source localization with a classification {DNN}. Different {HRTFs} are used to generate training and test sets, respectively. On this basis, we study the localization performance of {DNN} model trained by each training set on different test sets. The result shows that the localization performance of the same model on different test sets is different, while the localization performance of different models on the same test set may be similar. The result also shows a clustering trend. Secondly, different {HRTFs} are divided into several clusters. Finally, the corresponding {HRTFs} of each cluster center are selected to generate a new training set and to train a more generalized {DNN} model. The experimental results show that the proposed method achieves better generalization performance than the baseline methods in the mismatched {HRTF} condition and has almost equal performance to the {DNN} trained with a large number of {HRTFs}, which means the proposed method is data-efficient.},
  pages        = {4},
  number       = {1},
  journaltitle = {{EURASIP} Journal on Audio, Speech, and Music Processing},
  shortjournal = {{EURASIP} Journal on Audio, Speech, and Music Processing},
  author       = {Wang, Jing and Wang, Jin and Qian, Kai and Xie, Xiang and Kuang, Jingming},
  urldate      = {2024-06-07},
  date         = {2020-02-10},
  keywords     = {Affinity propagation, Binaural localization, Clustering, Deep neural network},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/AXJ7NQJI/Wang et al. - 2020 - Binaural sound localization based on deep neural n.pdf:application/pdf;Snapshot:/Users/pawel/Zotero/storage/HXP2Q4IJ/s13636-020-0171-y.html:text/html}
}


@article{liu_multiple_2018,
  title        = {Multiple Speaker Tracking in Spatial Audio via {PHD} Filtering and Depth-Audio Fusion},
  volume       = {20},
  issn         = {1941-0077},
  url          = {https://ieeexplore.ieee.org/document/8119824},
  doi          = {10.1109/TMM.2017.2777671},
  abstract     = {In the object-based spatial audio system, positions of the audio objects (e.g., speakers/talkers or voices) presented in the sound scene are required as important metadata attributes for object acquisition and reproduction. Binaural microphones are often used as a physical device to mimic human hearing and to monitor and analyze the scene, including localization and tracking of multiple speakers. The binaural audio tracker, however, is usually prone to the errors caused by room reverberation and background noise. To address this limitation, we present a multimodal tracking method by fusing the binaural audio with depth information (from a depth sensor, e.g., Kinect). More specifically, the probability hypothesis density ({PHD}) filtering framework is first applied to the depth stream, and a novel clutter intensity model is proposed to improve the robustness of the {PHD} filter when an object is occluded either by other objects or due to the limited field of view of the depth sensor. To compensate misdetections in the depth stream, a novel gap filling technique is presented to map audio azimuths obtained from the binaural audio tracker to 3D positions, using speaker-dependent spatial constraints learned from the depth stream. With our proposed method, both the errors in the binaural tracker and the misdetections in the depth tracker can be significantly reduced. Real-room recordings are used to show the improved performance of the proposed method in removing outliers and reducing misdetections.},
  pages        = {1767--1780},
  number       = {7},
  journaltitle = {{IEEE} Transactions on Multimedia},
  author       = {Liu, Qingju and Wang, Wenwu and de Campos, Teófilo and Jackson, Philip J. B. and Hilton, Adrian},
  urldate      = {2024-06-07},
  date         = {2018-07},
  note         = {Conference Name: {IEEE} Transactions on Multimedia},
  keywords     = {Azimuth, binaural microphones, Clutter, depth and audio, depth sensor, Metadata, Microphones, Multi-person tracking, {PHD} filtering, spatial audio, Target tracking, Three-dimensional displays, Trajectory},
  file         = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/ETBEG7F9/8119824.html:text/html}
}


@article{ma_robust_2018,
  title        = {Robust Binaural Localization of a Target Sound Source by Combining Spectral Source Models and Deep Neural Networks},
  volume       = {26},
  issn         = {2329-9304},
  url          = {https://ieeexplore.ieee.org/document/8410799},
  doi          = {10.1109/TASLP.2018.2855960},
  abstract     = {Despite there being a clear evidence for top-down (e.g., attentional) effects in biological spatial hearing, relatively few machine hearing systems exploit the top-down model-based knowledge in sound localization. This paper addresses this issue by proposing a novel framework for the binaural sound localization that combines the model-based information about the spectral characteristics of sound sources and deep neural networks ({DNNs}). A target source model and a background source model are first estimated during a training phase using spectral features extracted from sound signals in isolation. When the identity of the background source is not available, a universal background model can be used. During testing, the source models are used jointly to explain the mixed observations and improve the localization process by selectively weighting source azimuth posteriors output by a {DNN}-based localization system. To address the possible mismatch between the training and testing, a model adaptation process is further employed the on-the-fly during testing, which adapts the background model parameters directly from the noisy observations in an iterative manner. The proposed system, therefore, combines the model-based and data-driven information flow within a single computational framework. The evaluation task involved localization of a target speech source in the presence of an interfering source and room reverberation. Our experiments show that by exploiting the model-based information in this way, the sound localization performance can be improved substantially under various noisy and reverberant conditions.},
  pages        = {2122--2131},
  number       = {11},
  journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  author       = {Ma, Ning and Gonzalez, Jose A. and Brown, Guy J.},
  urldate      = {2024-06-07},
  date         = {2018-11},
  note         = {Conference Name: {IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  keywords     = {Adaptation models, Auditory system, Azimuth, Binaural source localisation, Biological system modeling, Computational modeling, machine hearing, masking, reverberation, sound source combination, Speech processing, Time-frequency analysis},
  file         = {Accepted Version:/Users/pawel/Zotero/storage/LZQN43AY/Ma et al. - 2018 - Robust Binaural Localization of a Target Sound Sou.pdf:application/pdf}
}

# our

@article{antoniuk2023blind,
  author  = {antoniuk  pawel and zielinski  slawomir krzysztof},
  journal = {journal of the audio engineering society},
  title   = {blind estimation of ensemble width in binaural music recordings using 'spatiograms' under simulated anechoic conditions},
  year    = {2023},
  number  = {15},
  month   = {4}
}


@article{zielinski_automatic_2022,
  title        = {Automatic discrimination between front and back ensemble locations in {HRTF}-convolved binaural recordings of music},
  volume       = {2022},
  issn         = {1687-4722},
  url          = {https://doi.org/10.1186/s13636-021-00235-2},
  doi          = {10.1186/s13636-021-00235-2},
  abstract     = {One of the greatest challenges in the development of binaural machine audition systems is the disambiguation between front and back audio sources, particularly in complex spatial audio scenes. The goal of this work was to develop a method for discriminating between front and back located ensembles in binaural recordings of music. To this end, 22, 496 binaural excerpts, representing either front or back located ensembles, were synthesized by convolving multi-track music recordings with 74 sets of head-related transfer functions ({HRTF}). The discrimination method was developed based on the traditional approach, involving hand-engineering of features, as well as using a deep learning technique incorporating the convolutional neural network ({CNN}). According to the results obtained under {HRTF}-dependent test conditions, {CNN} showed a very high discrimination accuracy (99.4\%), slightly outperforming the traditional method. However, under the {HRTF}-independent test scenario, {CNN} performed worse than the traditional algorithm, highlighting the importance of testing the algorithms under {HRTF}-independent conditions and indicating that the traditional method might be more generalizable than {CNN}. A minimum of 20 {HRTFs} are required to achieve a satisfactory generalization performance for the traditional algorithm and 30 {HRTFs} for {CNN}. The minimum duration of audio excerpts required by both the traditional and {CNN}-based methods was assessed as 3 s. Feature importance analysis, based on a gradient attribution mapping technique, revealed that for both the traditional and the deep learning methods, a frequency band between 5 and 6 {kHz} is particularly important in terms of the discrimination between front and back ensemble locations. Linear-frequency cepstral coefficients, interaural level differences, and audio bandwidth were identified as the key descriptors facilitating the discrimination process using the traditional approach.},
  pages        = {3},
  number       = {1},
  journaltitle = {{EURASIP} Journal on Audio, Speech, and Music Processing},
  shortjournal = {{EURASIP} Journal on Audio, Speech, and Music Processing},
  author       = {Zieliński, Sławomir K. and Antoniuk, Paweł and Lee, Hyunkook and Johnson, Dale},
  urldate      = {2024-06-10},
  date         = {2022-01-15},
  keywords     = {Binaural recordings, Feature engineering, {HRTF}, Spatial audio information retrieval},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/4G5WL8AM/Zieliński et al. - 2022 - Automatic discrimination between front and back en.pdf:application/pdf;Snapshot:/Users/pawel/Zotero/storage/SI2K4EJ6/s13636-021-00235-2.html:text/html}
}


@article{zielinski_spatial_2022,
  title        = {Spatial Audio Scene Characterization ({SASC}): Automatic Localization of Front-, Back-, Up-, and Down-Positioned Music Ensembles in Binaural Recordings},
  volume       = {12},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2076-3417},
  url          = {https://www.mdpi.com/2076-3417/12/3/1569},
  doi          = {10.3390/app12031569},
  shorttitle   = {Spatial Audio Scene Characterization ({SASC})},
  abstract     = {The automatic localization of audio sources distributed symmetrically with respect to coronal or transverse planes using binaural signals still poses a challenging task, due to the front–back and up–down confusion effects. This paper demonstrates that the convolutional neural network ({CNN}) can be used to automatically localize music ensembles panned to the front, back, up, or down positions. The network was developed using the repository of the binaural excerpts obtained by the convolution of multi-track music recordings with the selected sets of head-related transfer functions ({HRTFs}). They were generated in such a way that a music ensemble (of circular shape in terms of its boundaries) was positioned in one of the following four locations with respect to the listener: front, back, up, and down. According to the obtained results, {CNN} identified the location of the ensembles with the average accuracy levels of 90.7\% and 71.4\% when tested under the {HRTF}-dependent and {HRTF}-independent conditions, respectively. For {HRTF}-dependent tests, the accuracy decreased monotonically with the increase in the ensemble size. A modified image occlusion sensitivity technique revealed selected frequency bands as being particularly important in terms of the localization process. These frequency bands are largely in accordance with the psychoacoustical literature.},
  pages        = {1569},
  number       = {3},
  journaltitle = {Applied Sciences},
  author       = {Zieliński, Sławomir K. and Antoniuk, Paweł and Lee, Hyunkook},
  urldate      = {2024-06-10},
  date         = {2022-01},
  langid       = {english},
  note         = {Number: 3
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {convolutional neural networks, deep learning, spatial audio information retrieval, spatial audio scene characterization},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/HDGFITDN/Zieliński et al. - 2022 - Spatial Audio Scene Characterization (SASC) Autom.pdf:application/pdf}
}


@article{zielinski_comparison_2020,
  title        = {A Comparison of Human against Machine-Classification of Spatial Audio Scenes in Binaural Recordings of Music},
  volume       = {10},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2076-3417},
  url          = {https://www.mdpi.com/2076-3417/10/17/5956},
  doi          = {10.3390/app10175956},
  abstract     = {The purpose of this paper is to compare the performance of human listeners against the selected machine learning algorithms in the task of the classification of spatial audio scenes in binaural recordings of music under practical conditions. The three scenes were subject to classification: (1) music ensemble (a group of musical sources) located in the front, (2) music ensemble located at the back, and (3) music ensemble distributed around a listener. In the listening test, undertaken remotely over the Internet, human listeners reached the classification accuracy of 42.5\%. For the listeners who passed the post-screening test, the accuracy was greater, approaching 60\%. The above classification task was also undertaken automatically using four machine learning algorithms: convolutional neural network, support vector machines, extreme gradient boosting framework, and logistic regression. The machine learning algorithms substantially outperformed human listeners, with the classification accuracy reaching 84\%, when tested under the binaural-room-impulse-response ({BRIR}) matched conditions. However, when the algorithms were tested under the {BRIR} mismatched scenario, the accuracy obtained by the algorithms was comparable to that exhibited by the listeners who passed the post-screening test, implying that the machine learning algorithms capability to perform in unknown electro-acoustic conditions needs to be further improved.},
  pages        = {5956},
  number       = {17},
  journaltitle = {Applied Sciences},
  author       = {Zieliński, Sławomir K. and Lee, Hyunkook and Antoniuk, Paweł and Dadan, Oskar},
  urldate      = {2024-06-10},
  date         = {2020-01},
  langid       = {english},
  note         = {Number: 17
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {convolutional neural networks, deep learning, spatial audio information retrieval, spatial audio scene classification},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/SC5RHNYP/Zieliński et al. - 2020 - A Comparison of Human against Machine-Classificati.pdf:application/pdf}
}


# glass-box methods

# black-box methods

