%  introduction

@article{cherry_experiments_1953,
  title        = {Some experiments on the recognition of speech, with one and with two ears.},
  volume       = {25},
  issn         = {0001-4966(Print)},
  doi          = {10.1121/1.1907229},
  abstract     = {Experiments are described which examine the separation of two speech signals by human operators. 3 procedures were employed: presentation of both messages to both ears; presentation of one message to one ear with simultaneous presentation of the second message to the other ear; and presentation of a single message alternately to the two ears. Of particular current interest is the second procedure. Here, a listener is able to separate the messages exceedingly well despite the fact that he can only identify the general statistical properties of the rejected message. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
  pages        = {975--979},
  journaltitle = {Journal of the Acoustical Society of America},
  author       = {Cherry, E. Colin},
  date         = {1953},
  note         = {Place: {US}
                  Publisher: Acoustical Society of American}
}


@article{hirsh_binaural_1950,
  title        = {Binaural Hearing Aids: A Review Of Some Experiments},
  volume       = {15},
  issn         = {0022-4677, 2163-6184},
  url          = {http://pubs.asha.org/doi/10.1044/jshd.1502.114},
  doi          = {10.1044/jshd.1502.114},
  shorttitle   = {Binaural Hearing Aids},
  pages        = {114--123},
  number       = {2},
  journaltitle = {Journal of Speech and Hearing Disorders},
  shortjournal = {J Speech Hear Disord},
  author       = {Hirsh, Ira J.},
  urldate      = {2024-06-10},
  date         = {1950-06},
  langid       = {english},
  file         = {Hirsh - 1950 - Binaural Hearing Aids A Review Of Some Experiment.pdf:/Users/pawel/Zotero/storage/WI3CH9LM/Hirsh - 1950 - Binaural Hearing Aids A Review Of Some Experiment.pdf:application/pdf}
}

@article{thiemann_speech_2016,
  title    = {Speech enhancement for multimicrophone binaural hearing aids aiming to preserve the spatial auditory scene},
  volume   = {2016},
  issn     = {1687-6180},
  url      = {https://doi.org/10.1186/s13634-016-0314-6},
  doi      = {10.1186/s13634-016-0314-6},
  abstract = {Modern binaural hearing aids utilize multimicrophone speech enhancement algorithms to enhance signals in terms of signal-to-noise ratio, but they may distort the interaural cues that allow the user to localize sources, in particular, suppressed interfering sources or background noise. In this paper, we present a novel algorithm that enhances the target signal while aiming to maintain the correct spatial rendering of both the target signal as well as the background noise. We use a bimodal approach, where a signal-to-noise ratio (SNR) estimator controls a binary decision mask, switching between the output signals of a binaural minimum variance distortionless response (MVDR) beamformer and scaled reference microphone signals. We show that the proposed selective binaural beamformer (SBB) can enhance the target signal while maintaining the overall spatial rendering of the acoustic scene.},
  number   = {1},
  urldate  = {2024-06-07},
  journal  = {EURASIP Journal on Advances in Signal Processing},
  author   = {Thiemann, Joachim and Müller, Menno and Marquardt, Daniel and Doclo, Simon and van de Par, Steven},
  month    = feb,
  year     = {2016},
  keywords = {Bilateral hearing aids, Binaural hearing aids, Binaural MVDR, Hearing aids},
  pages    = {12},
  file     = {Full Text PDF:/Users/pawel/Zotero/storage/HK2UGN3E/Thiemann et al. - 2016 - Speech enhancement for multimicrophone binaural he.pdf:application/pdf;Snapshot:/Users/pawel/Zotero/storage/MPJJ7SUT/s13634-016-0314-6.html:text/html}
}

@book{blauert_spatial_1996,
  title     = {Spatial {Hearing}: {The} {Psychophysics} of {Human} {Sound} {Localization}},
  isbn      = {978-0-262-26868-4},
  url       = {https://doi.org/10.7551/mitpress/6391.001.0001},
  abstract  = {The field of spatial hearing has exploded in the decade or so since Jens Blauert's classic work on acoustics was first published in English. This revised edition adds a new chapter that describes developments in such areas as auditory virtual reality (an important field of application that is based mainly on the physics of spatial hearing), binaural technology (modeling speech enhancement by binaural hearing), and spatial sound-field mapping. The chapter also includes recent research on the precedence effect that provides clear experimental evidence that cognition plays a significant role in spatial hearing. The remaining four chapters in this comprehensive reference cover auditory research procedures and psychometric methods, spatial hearing with one sound source, spatial hearing with multiple sound sources and in enclosed spaces, and progress and trends from 1972 (the first German edition) to 1983 (the first English edition)—work that includes research on the physics of the external ear, and the application of signal processing theory to modeling the spatial hearing process. There is an extensive bibliography of more than 900 items.},
  publisher = {The MIT Press},
  author    = {Blauert, Jens},
  month     = oct,
  year      = {1996},
  doi       = {10.7551/mitpress/6391.001.0001}
}


@article{zhang_surround_2017,
  title      = {Surround by {Sound}: {A} {Review} of {Spatial} {Audio} {Recording} and {Reproduction}},
  volume     = {7},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  issn       = {2076-3417},
  shorttitle = {Surround by {Sound}},
  url        = {https://www.mdpi.com/2076-3417/7/5/532},
  doi        = {10.3390/app7050532},
  abstract   = {In this article, a systematic overview of various recording and reproduction techniques for spatial audio is presented. While binaural recording and rendering is designed to resemble the human two-ear auditory system and reproduce sounds specifically for a listener’s two ears, soundfield recording and reproduction using a large number of microphones and loudspeakers replicate an acoustic scene within a region. These two fundamentally different types of techniques are discussed in the paper. A recent popular area, multi-zone reproduction, is also briefly reviewed in the paper. The paper is concluded with a discussion of the current state of the field and open problems.},
  language   = {en},
  number     = {5},
  urldate    = {2024-06-07},
  journal    = {Applied Sciences},
  author     = {Zhang, Wen and Samarasinghe, Parasanga N. and Chen, Hanchi and Abhayapala, Thushara D.},
  month      = may,
  year       = {2017},
  note       = {Number: 5
                Publisher: Multidisciplinary Digital Publishing Institute},
  keywords   = {binaural recording, binaural rendering, multi-zone reproduction, soundfield recording, soundfield reproduction, spatial audio},
  pages      = {532},
  file       = {Full Text PDF:/Users/pawel/Zotero/storage/8AJ5DLMH/Zhang et al. - 2017 - Surround by Sound A Review of Spatial Audio Recor.pdf:application/pdf}
}


@inproceedings{yang_deepear_2022,
  title      = {{DeepEar}: {Sound} {Localization} with {Binaural} {Microphones}},
  shorttitle = {{DeepEar}},
  url        = {https://ieeexplore.ieee.org/document/9796850},
  doi        = {10.1109/INFOCOM48880.2022.9796850},
  abstract   = {Binaural microphones, referring to two microphones with artificial human-shaped ears, are pervasively used in humanoid robots and hearing aids improving sound quality. In many applications, it is crucial for such robots to interact with humans by finding the voice direction. However, sound source localization with binaural microphones remains challenging, especially in multi-source scenarios. Prior works utilize microphone arrays to deal with the multi-source localization problem. Extra arrays yet incur higher deployment costs and take up more space. However, human brains have evolved to locate multiple sound sources with only two ears. Inspired by this fact, we propose DeepEar, a binaural microphone-based localization system that can locate multiple sounds. To this end, we develop a neural network to mimic the acoustic signal processing pipeline of the human auditory system. Different from hand-crafted features used in prior works, DeepEar can automatically extract useful features for localization. More importantly, the trained neural networks can be extended and adapted to new environments with a minimum amount of extra training data. Experiment results show that DeepEar can substantially outperform the state-of-the-art deep learning approach, with a sound detection accuracy of 93.3\% and an azimuth estimation error of 7.4 degrees in multisource scenarios.},
  urldate    = {2024-06-07},
  booktitle  = {{IEEE} {INFOCOM} 2022 - {IEEE} {Conference} on {Computer} {Communications}},
  author     = {Yang, Qiang and Zheng, Yuanqing},
  month      = may,
  year       = {2022},
  note       = {ISSN: 2641-9874},
  keywords   = {Auditory system, Binaural localization, Deep learning, Ear, Earable computing, Feature extraction, Location awareness, Multi-source localization, Training data, Transfer learning},
  pages      = {960--969},
  file       = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/IN9CQI5D/9796850.html:text/html}
}

%  groups

@incollection{bregman_auditory_1990,
  title      = {Auditory {Scene} {Analysis}: {The} {Perceptual} {Organization} of {Sound}},
  volume     = {95},
  shorttitle = {Auditory {Scene} {Analysis}},
  abstract   = {Scitation is the online home of leading journals and conference proceedings from AIP Publishing and AIP Member Societies},
  booktitle  = {Journal of {The} {Acoustical} {Society} of {America} - {J} {ACOUST} {SOC} {AMER}},
  author     = {Bregman, Albert},
  month      = jan,
  year       = {1990},
  doi        = {10.1121/1.408434},
  note       = {Journal Abbreviation: Journal of The Acoustical Society of America - J ACOUST SOC AMER},
  file       = {Full Text PDF:/Users/pawel/Zotero/storage/SR53ZIS7/Bregman - 1990 - Auditory Scene Analysis The Perceptual Organizati.pdf:application/pdf},
  publisher  = {MIT Press}
}


@article{rumsey_spatial_2002,
  title        = {Spatial Quality Evaluation for Reproduced Sound: Terminology, Meaning, and a Scene-Based Paradigm},
  volume       = {50},
  shorttitle   = {Spatial Quality Evaluation for Reproduced Sound},
  abstract     = {Spatial quality in reproduced sound is a subset of the broad topic of sound quality. In the past it has been studied less rigorously than other aspects of reproduced sound quality, leading to a lack of clarity in standard definitions of subjective attributes. Rigor in the physical measurement of sound signals should be matched by equal rigor in semantics relating to subjective evaluation. A scene-based paradigm for the description and assessment of spatial quality is described, which enables clear distinctions to be made between elements of a reproduced sound scene and will assist in the search for related physical parameters.},
  pages        = {651--666},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {Journal of the Audio Engineering Society},
  author       = {Rumsey, Francis},
  date         = {2002-09-01}
}

@article{rumsey_spatial_2002,
  title        = {Spatial Quality Evaluation for Reproduced Sound: Terminology, Meaning, and a Scene-Based Paradigm},
  volume       = {50},
  shorttitle   = {Spatial Quality Evaluation for Reproduced Sound},
  abstract     = {Spatial quality in reproduced sound is a subset of the broad topic of sound quality. In the past it has been studied less rigorously than other aspects of reproduced sound quality, leading to a lack of clarity in standard definitions of subjective attributes. Rigor in the physical measurement of sound signals should be matched by equal rigor in semantics relating to subjective evaluation. A scene-based paradigm for the description and assessment of spatial quality is described, which enables clear distinctions to be made between elements of a reproduced sound scene and will assist in the search for related physical parameters.},
  pages        = {651--666},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {Journal of the Audio Engineering Society},
  author       = {Rumsey, Francis},
  date         = {2002-09-01}
}

%  related studies

%  multi-channel


@article{kaveh_statistical_1986,
  title        = {The statistical performance of the {MUSIC} and the minimum-norm algorithms in resolving plane waves in noise},
  volume       = {34},
  issn         = {0096-3518},
  url          = {https://ieeexplore.ieee.org/document/1164815},
  doi          = {10.1109/TASSP.1986.1164815},
  abstract     = {This paper presents an asymptotic statistical analysis of the null-spectra of two eigen-assisted methods, {MUSIC} [1] and Minimum-Norm [2], for resolving independent closely spaced plane waves in noise. Particular attention is paid to the average deviation of the null-spectra from zero at the true angles of arrival for the plane waves. These deviations are expressed as functions of signal-to-noise ratios, number of array elements, angular separation of emitters, and the number of snapshots. In the case of {MUSIC}. an approximate expression is derived for the resolution threshold of two plane waves with equal power in noise. This result is validated by Monte Carlo simulations.},
  pages        = {331--341},
  number       = {2},
  journaltitle = {{IEEE} Transactions on Acoustics, Speech, and Signal Processing},
  author       = {Kaveh, M. and Barabell, A.},
  urldate      = {2024-06-07},
  date         = {1986-04},
  note         = {Conference Name: {IEEE} Transactions on Acoustics, Speech, and Signal Processing},
  keywords     = {Covariance matrix, Direction of arrival estimation, Monte Carlo methods, Multiple signal classification, Narrowband, Signal resolution, Signal to noise ratio, Spatial resolution, Spectral analysis, Statistical analysis}
}





@inproceedings{pavlidi_real-time_2012,
  title     = {Real-time multiple sound source localization using a circular microphone array based on single-source confidence measures},
  url       = {https://ieeexplore.ieee.org/document/6288455},
  doi       = {10.1109/ICASSP.2012.6288455},
  abstract  = {We propose a novel real-time adaptative localization approach for multiple sources using a circular array, in order to suppress the localization ambiguities faced with linear arrays, and assuming a weak sound source sparsity which is derived from blind source separation methods. Our proposed method performs very well both in simulations and in real conditions at 50\% real-time.},
  pages     = {2625--2628},
  booktitle = {2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  author    = {Pavlidi, Despoina and Puigt, Matthieu and Griffin, Anthony and Mouchtaris, Athanasios},
  urldate   = {2024-06-07},
  date      = {2012-03},
  note      = {{ISSN}: 2379-190X},
  keywords  = {Speech, Microphones, Estimation, Direction of arrival estimation, multiple source localization, Array signal processing, Arrays, direction of arrival estimation, Real time systems, Time frequency analysis},
  file      = {Full Text:/home/pawel/Zotero/storage/FXAAV3EW/Pavlidi et al. - 2012 - Real-time multiple sound source localization using.pdf:application/pdf;IEEE Xplore Abstract Record:/home/pawel/Zotero/storage/RI3ZVKSY/6288455.html:text/html}
}




@article{pan_multi-tone_2021,
  title        = {Multi-Tone Phase Coding of Interaural Time Difference for Sound Source Localization With Spiking Neural Networks},
  volume       = {29},
  issn         = {2329-9304},
  url          = {https://ieeexplore.ieee.org/document/9502013},
  doi          = {10.1109/TASLP.2021.3100684},
  abstract     = {Mammals exhibit remarkable capability of detecting and localizing sound sources in complex acoustic environments by using binaural cues in the spiking manner. Emulating the auditory process for sound source localization ({SSL}) by mammals, we propose a computational model for accurate and robust {SSL} under the neuromorphic spiking neural network ({SNN}) framework. The center of this model is a Multi-Tone Phase Coding ({MTPC}) scheme, which encodes the interaural time difference ({ITD}) between binaural pure tones into discriminative spike patterns that can be directly classified by {SNNs}. As such, {SSL} can be implemented as an event-driven task on highly efficient, neuromorphic parallel processors. We evaluate the proposed computational model on a directional audio dataset recorded from a microphone array in a realistic acoustic environment with background noise, obstruction, reflection, and other interferences. We report superior localization capability with a mean absolute error ({MAE}) of 1.02° or 100\% classification accuracy with an angle resolution of 5°, which surpasses other {SNN}-based biologically plausible neuromorphic approaches by a relatively large margin and on par with human performance in similar tasks. This study opens up many application opportunities in human-robot interaction where energy efficiency is crucial. As a case study, we successfully deploy the proposed {SSL} system in a robotic platform to track the speaker and orient the robot's attention.},
  pages        = {2656--2670},
  journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  author       = {Pan, Zihan and Zhang, Malu and Wu, Jibin and Wang, Jiadong and Li, Haizhou},
  urldate      = {2024-06-07},
  date         = {2021},
  note         = {Conference Name: {IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  keywords     = {Acoustics, Biological neural networks, Computational modeling, Ear, Encoding, Location awareness, Neural phase coding, Neurons, sound source localization, spiking neural network},
  file         = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/8CJB3YFJ/9502013.html:text/html}
}



@article{hahmann_sound_2022,
  title        = {Sound source localization using multiple ad hoc distributed microphone arrays},
  volume       = {2},
  issn         = {2691-1191},
  doi          = {10.1121/10.0011811},
  abstract     = {Sound source localization is crucial for communication and sound scene analysis. This study uses direction-of-arrival estimates of multiple ad hoc distributed microphone arrays to localize sound sources in a room. An affine mapping between the independent array estimates and the source coordinates is derived from a set of calibration points. Experiments show that the affine model is sufficient to locate a source and can be calibrated to physical dimensions. A projection of the local array estimates increases localization accuracy, particularly further away from the calibrated region. Localization tests in three dimensions compare the affine approach to a nonlinear neural network.},
  pages        = {074801},
  number       = {7},
  journaltitle = {{JASA} express letters},
  shortjournal = {{JASA} Express Lett},
  author       = {Hahmann, Manuel and Fernandez-Grande, Efren and Gunawan, Henrry and Gerstoft, Peter},
  date         = {2022-07},
  pmid         = {36154052},
  keywords     = {Acoustics, Sound, Sound Localization},
  file         = {Full Text:/Users/pawel/Zotero/storage/5MD884GP/Hahmann et al. - 2022 - Sound source localization using multiple ad hoc di.pdf:application/pdf}
}



@article{chung_sound_2022,
  title        = {Sound Localization Based on Acoustic Source Using Multiple Microphone Array in an Indoor Environment},
  volume       = {11},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2079-9292},
  url          = {https://www.mdpi.com/2079-9292/11/6/890},
  doi          = {10.3390/electronics11060890},
  abstract     = {Sound signals have been widely applied in various fields. One of the popular applications is sound localization, where the location and direction of a sound source are determined by analyzing the sound signal. In this study, two microphone linear arrays were used to locate the sound source in an indoor environment. The {TDOA} is also designed to deal with the problem of delay in the reception of sound signals from two microphone arrays by using the generalized cross-correlation algorithm to calculate the {TDOA}. The proposed microphone array system with the algorithm can successfully estimate the sound source’s location. The test was performed in a standardized chamber. This experiment used two microphone arrays, each with two microphones. The experimental results prove that the proposed method can detect the sound source and obtain good performance with a position error of about 2.0{\textasciitilde}2.3 cm and angle error of about 0.74 degrees. Therefore, the experimental results demonstrate the feasibility of the system.},
  pages        = {890},
  number       = {6},
  journaltitle = {Electronics},
  author       = {Chung, Ming-An and Chou, Hung-Chi and Lin, Chia-Wei},
  urldate      = {2024-06-07},
  date         = {2022-01},
  langid       = {english},
  note         = {Number: 6
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {generalized cross-correlation algorithm, indoor localization, microphone array, sound localization, time difference of arrival},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/PX2QWICQ/Chung et al. - 2022 - Sound Localization Based on Acoustic Source Using .pdf:application/pdf}
}



@article{liu_sound_2022,
  title        = {Sound Source Localization Based on Multi-Channel Cross-Correlation Weighted Beamforming},
  volume       = {13},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2072-666X},
  url          = {https://www.mdpi.com/2072-666X/13/7/1010},
  doi          = {10.3390/mi13071010},
  abstract     = {Beamforming and its applications in steered-response power ({SRP}) technology, such as steered-response power delay and sum ({SRP}-{DAS}) and steered-response power phase transform ({SRP}-{PHAT}), are widely used in sound source localization. However, their resolution and accuracy still need improvement. A novel beamforming method combining {SRP} and multi-channel cross-correlation coefficient ({MCCC}), {SRP}-{MCCC}, is proposed in this paper to improve the accuracy of direction of arrival ({DOA}). Directional weight ({DW}) is obtained by calculating the {MCCC}. Based on {DW}, suppressed the non-incoming wave direction and gained the incoming wave direction to improve the beamforming capabilities. Then, sound source localizations based on the dual linear array under different conditions were simulated. Compared with {SRP}-{PHAT}, {SRP}-{MCCC} has the advantages of high positioning accuracy, strong spatial directivity and robustness under the different signal–noise ratios ({SNRs}). When the {SNR} is −10 {dB}, the average positioning error of the single-frequency sound source at different coordinates decreases by 5.69\%, and that of the mixed frequency sound sources at the same coordinate decreases by 5.77\%. Finally, the experimental verification was carried out. The results show that the average error of {SRP}-{MCCC} has been reduced by 8.14\% and the positioning accuracy has been significantly improved, which is consistent with the simulation results. This research provides a new idea for further engineering applications of sound source localization based on beamforming.},
  pages        = {1010},
  number       = {7},
  journaltitle = {Micromachines},
  author       = {Liu, Mengran and Hu, Junhao and Zeng, Qiang and Jian, Zeming and Nie, Lei},
  urldate      = {2024-06-07},
  date         = {2022-07},
  langid       = {english},
  note         = {Number: 7
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {beamforming, microphone array, multi-channel cross-correlation coefficient, sound source localization},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/J597NXKN/Liu et al. - 2022 - Sound Source Localization Based on Multi-Channel C.pdf:application/pdf}
}



%  individual sources


@article{dietz_auditory_2011,
  series   = {Perceptual and {Statistical} {Audition}},
  title    = {Auditory model based direction estimation of concurrent speakers from binaural signals},
  volume   = {53},
  issn     = {0167-6393},
  url      = {https://www.sciencedirect.com/science/article/pii/S016763931000097X},
  doi      = {10.1016/j.specom.2010.05.006},
  abstract = {Humans show a very robust ability to localize sounds in adverse conditions. Computational models of binaural sound localization and technical approaches of direction-of-arrival (DOA) estimation also show good performance, however, both their binaural feature extraction and the strategies for further analysis partly differ from what is currently known about the human auditory system. This study investigates auditory model based DOA estimation emphasizing known features and limitations of the auditory binaural processing such as (i) high temporal resolution, (ii) restricted frequency range to exploit temporal fine-structure, (iii) use of temporal envelope disparities, and (iv) a limited range to compensate for interaural time delay. DOA estimation performance was investigated for up to five concurrent speakers in free field and for up to three speakers in the presence of noise. The DOA errors in these conditions were always smaller than 5°. A condition with moving speakers was also tested and up to three moving speakers could be tracked simultaneously. Analysis of DOA performance as a function of the binaural temporal resolution showed that short time constants of about 5ms employed by the auditory model were crucial for robustness against concurrent sources.},
  number   = {5},
  urldate  = {2024-06-07},
  journal  = {Speech Communication},
  author   = {Dietz, Mathias and Ewert, Stephan D. and Hohmann, Volker},
  month    = may,
  year     = {2011},
  keywords = {Auditory modeling, Binaural processing, Direction estimation},
  pages    = {592--605},
  file     = {ScienceDirect Snapshot:/Users/pawel/Zotero/storage/B6MXZWGK/S016763931000097X.html:text/html}
}


@article{may_probabilistic_2011,
  title    = {A {Probabilistic} {Model} for {Robust} {Localization} {Based} on a {Binaural} {Auditory} {Front}-{End}},
  volume   = {19},
  issn     = {1558-7924},
  url      = {https://ieeexplore.ieee.org/document/5406118},
  doi      = {10.1109/TASL.2010.2042128},
  abstract = {Although extensive research has been done in the field of machine-based localization, the degrading effect of reverberation and the presence of multiple sources on localization performance has remained a major problem. Motivated by the ability of the human auditory system to robustly analyze complex acoustic scenes, the associated peripheral stage is used in this paper as a front-end to estimate the azimuth of sound sources based on binaural signals. One classical approach to localize an acoustic source in the horizontal plane is to estimate the interaural time difference (ITD) between both ears by searching for the maximum in the cross-correlation function. Apart from ITDs, the interaural level difference (ILD) can contribute to localization, especially at higher frequencies where the wavelength becomes smaller than the diameter of the head, leading to ambiguous ITD information. The interdependency of ITD and ILD on azimuth is a complex pattern that depends also on the room acoustics, and is therefore learned by azimuth-dependent Gaussian mixture models (GMMs). Multiconditional training is performed to take into account the variability of the binaural features which results from multiple sources and the effect of reverberation. The proposed localization model outperforms state-of-the-art localization techniques in simulated adverse acoustic conditions.},
  number   = {1},
  urldate  = {2024-06-07},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {May, Tobias and van de Par, Steven and Kohlrausch, Armin},
  month    = jan,
  year     = {2011},
  note     = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {auditory scene analysis (ASA), Auditory system, Azimuth, binaural, Degradation, Ear, Frequency, Humans, interaural level difference (ILD), interaural time difference (ITD), Layout, Localization, reverberation, Reverberation, Robustness, Signal analysis},
  pages    = {1--13}
}


@article{may_binaural_2012,
  title    = {A {Binaural} {Scene} {Analyzer} for {Joint} {Localization} and {Recognition} of {Speakers} in the {Presence} of {Interfering} {Noise} {Sources} and {Reverberation}},
  volume   = {20},
  issn     = {1558-7924},
  url      = {https://ieeexplore.ieee.org/document/6178270},
  doi      = {10.1109/TASL.2012.2193391},
  abstract = {In this study, we present a binaural scene analyzer that is able to simultaneously localize, detect and identify a known number of target speakers in the presence of spatially positioned noise sources and reverberation. In contrast to many other binaural cocktail party processors, the proposed system does not require a priori knowledge about the azimuth position of the target speakers. The proposed system consists of three main building blocks: binaural localization, speech source detection, and automatic speaker identification. First, a binaural front-end is used to robustly localize relevant sound source activity. Second, a speech detection module based on missing data classification is employed to determine whether detected sound source activity corresponds to a speaker or to an interfering noise source using a binary mask that is based on spatial evidence supplied by the binaural front-end. Third, a second missing data classifier is used to recognize the speaker identities of all detected speech sources. The proposed system is systematically evaluated in simulated adverse acoustic scenarios. Compared to state-of-the art MFCC recognizers, the proposed model achieves significant speaker recognition accuracy improvements.},
  number   = {7},
  urldate  = {2024-06-07},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {May, Tobias and van de Par, Steven and Kohlrausch, Armin},
  month    = sep,
  year     = {2012},
  note     = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {Acoustics, Auditory system, Automatic speaker recognition, binaural processing, computational auditory scene analysis (CASA), Humans, mask estimation, missing data, Noise, Speech, Speech recognition, Target recognition},
  pages    = {2016--2030}
}


@article{woodruff_binaural_2012,
  title    = {Binaural {Localization} of {Multiple} {Sources} in {Reverberant} and {Noisy} {Environments}},
  volume   = {20},
  issn     = {1558-7924},
  url      = {https://ieeexplore.ieee.org/document/6129395},
  doi      = {10.1109/TASL.2012.2183869},
  abstract = {Sound source localization from a binaural input is a challenging problem, particularly when multiple sources are active simultaneously and reverberation or background noise are present. In this work, we investigate a multi-source localization framework in which monaural source segregation is used as a mechanism to increase the robustness of azimuth estimates from a binaural input. We demonstrate performance improvement relative to binaural only methods assuming a known number of spatially stationary sources. We also propose a flexible azimuth-dependent model of binaural features that independently captures characteristics of the binaural setup and environmental conditions, allowing for adaptation to new environments or calibration to an unseen binaural setup. Results with both simulated and recorded impulse responses show that robust performance can be achieved with limited prior training.},
  number   = {5},
  urldate  = {2024-06-07},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Woodruff, John and Wang, DeLiang},
  month    = jul,
  year     = {2012},
  note     = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {Adaptation models, Azimuth, Binaural sound localization, computational auditory scene analysis (CASA), Estimation, Feature extraction, monaural grouping, Noise measurement, reverberation, Reverberation, Time frequency analysis},
  pages    = {1503--1512},
  file     = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/9CW9FT7X/6129395.html:text/html}
}


@inproceedings{may_robust_2015,
  title     = {Robust localisation of multiple speakers exploiting head movements and multi-conditional training of binaural cues},
  url       = {https://ieeexplore.ieee.org/document/7178457},
  doi       = {10.1109/ICASSP.2015.7178457},
  abstract  = {This paper addresses the problem of localising multiple competing speakers in the presence of room reverberation, where sound sources can be positioned at any azimuth on the horizontal plane. To reduce the amount of front-back confusions which can occur due to the similarity of interaural time differences (ITDs) and interaural level differences (ILDs) in the front and rear hemifield, a machine hearing system is presented which combines supervised learning of binaural cues using multi-conditional training (MCT) with a head movement strategy. A systematic evaluation showed that this approach substantially reduced the amount of front-back confusions in challenging acoustic scenarios. Moreover, the system was able to generalise to a variety of different acoustic conditions not seen during training.},
  urldate   = {2024-06-07},
  booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author    = {May, Tobias and Ma, Ning and Brown, Guy J.},
  month     = apr,
  year      = {2015},
  note      = {ISSN: 2379-190X},
  keywords  = {Acoustics, Auditory system, Azimuth, binaural sound source localisation, generalisation, head movements, Magnetic heads, multi-conditional training, Robustness, Speech, Training},
  pages     = {2679--2683},
  file      = {Accepted Version:/Users/pawel/Zotero/storage/UUF7ABAN/May et al. - 2015 - Robust localisation of multiple speakers exploitin.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/VAZXWS4I/7178457.html:text/html}
}

@inproceedings{ma16c_interspeech,
  author    = {Ning Ma and Guy J. Brown},
  title     = {{Speech Localisation in a Multitalker Mixture by Humans and Machines}},
  year      = 2016,
  booktitle = {Proc. Interspeech 2016},
  pages     = {3359--3363},
  doi       = {10.21437/Interspeech.2016-1149},
  issn      = {2308-457X}
}

@article{benaroya_binaural_2018,
  title    = {Binaural {Localization} of {Multiple} {Sound} {Sources} by {Non}-{Negative} {Tensor} {Factorization}},
  volume   = {26},
  issn     = {2329-9304},
  url      = {https://ieeexplore.ieee.org/document/8294267},
  doi      = {10.1109/TASLP.2018.2806745},
  abstract = {This paper presents non-negative factorization of audio signals for the binaural localization of multiple sound sources within realistic and unknown sound environments. Non-negative tensor factorization (NTF) provides a sparse representation of multichannel audio signals in time, frequency, and space that can be exploited in computational audio scene analysis and robot audition for the separation and localization of sound sources. In the proposed formulation, each sound source is represented by means of spectral dictionaries, temporal activation, and its distribution within each channel (here, left and right ears). This distribution, being dependent on the frequency, can be interpreted as an explicit estimation of the Head-Related Transfer Function (HRTF) of a binaural head which can then be converted into the estimated sound source position. Moreover, the semisupervised formulation of the non-negative factorization allows us to integrate prior knowledge about some sound sources of interest whose dictionaries can be learned in advance, whereas the remaining sources are considered as background sound, which remains unknown and is estimated on the fly. The proposed NTF-based sound source localization is applied here to binaural sound source localization of multiple speakers within realistic sound environments.},
  number   = {6},
  urldate  = {2024-06-07},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Benaroya, Elie Laurent and Obin, Nicolas and Liuni, Marco and Roebel, Axel and Raumel, Wilson and Argentieri, Sylvain},
  month    = jun,
  year     = {2018},
  note     = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  keywords = {Binaural localization, computational audio scene analysis, Ear, Image analysis, non-negative tensor factorization, robot audition, Robot kinematics, Speech, Speech processing, Tensile stress},
  pages    = {1072--1082},
  file     = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/AGTH4VGH/8294267.html:text/html;Submitted Version:/Users/pawel/Zotero/storage/TERC7QZT/Benaroya et al. - 2018 - Binaural Localization of Multiple Sound Sources by.pdf:application/pdf}
}


%  assumed number of sources

@article{ma_exploiting_2017,
  title    = {Exploiting {Deep} {Neural} {Networks} and {Head} {Movements} for {Robust} {Binaural} {Localisation} of {Multiple} {Sources} in {Reverberant} {Environments}},
  volume   = {25},
  issn     = {2329-9290, 2329-9304},
  url      = {http://arxiv.org/abs/1904.03001},
  doi      = {10.1109/TASLP.2017.2750760},
  abstract = {This paper presents a novel machine-hearing system that exploits deep neural networks (DNNs) and head movements for robust binaural localisation of multiple sources in reverberant environments. DNNs are used to learn the relationship between the source azimuth and binaural cues, consisting of the complete cross-correlation function (CCF) and interaural level differences (ILDs). In contrast to many previous binaural hearing systems, the proposed approach is not restricted to localisation of sound sources in the frontal hemifield. Due to the similarity of binaural cues in the frontal and rear hemifields, front-back confusions often occur. To address this, a head movement strategy is incorporated in the localisation model to help reduce the front-back errors. The proposed DNN system is compared to a Gaussian mixture model (GMM) based system that employs interaural time differences (ITDs) and ILDs as localisation features. Our experiments show that the DNN is able to exploit information in the CCF that is not available in the ITD cue, which together with head movements substantially improves localisation accuracies under challenging acoustic scenarios in which multiple talkers and room reverberation are present.},
  number   = {12},
  urldate  = {2024-06-07},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Ma, Ning and May, Tobias and Brown, Guy J.},
  month    = dec,
  year     = {2017},
  note     = {arXiv:1904.03001 [cs, eess]},
  keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  pages    = {2444--2453},
  annote   = {Comment: 10 pages},
  file     = {arXiv Fulltext PDF:/Users/pawel/Zotero/storage/Q8KYT7PT/Ma et al. - 2017 - Exploiting Deep Neural Networks and Head Movements.pdf:application/pdf;arXiv.org Snapshot:/Users/pawel/Zotero/storage/NXKLQ8NH/1904.html:text/html}
}

@article{pang_multitask_2019,
  title    = {Multitask {Learning} of {Time}-{Frequency} {CNN} for {Sound} {Source} {Localization}},
  volume   = {7},
  issn     = {2169-3536},
  url      = {https://ieeexplore.ieee.org/document/8668414},
  doi      = {10.1109/ACCESS.2019.2905617},
  abstract = {Sound source localization (SSL) is an important technique for many audio processing systems, such as speech enhancement/recognition and human-robot interaction. Although many methods have been proposed for SSL, it still remains a challenging task to achieve accurate localization under adverse acoustic scenarios. In this paper, a novel binaural SSL method based on time-frequency convolutional neural network (TF-CNN) with multitask learning is proposed to simultaneously localize azimuth and elevation under unknown acoustic conditions. First, the interaural phase difference and interaural level difference are extracted from the received binaural signals, which are taken as the input of the proposed SSL neural network. Then, an SSL neural network is designed to map the interaural cues to sound direction, which consists of TF-CNN module and multitask neural network. The TF-CNN module learns and combines the time-frequency information of extracted interaural cues to generate the shared feature for multitask SSL. With the shared feature, a multitask neural network is designed to simultaneously estimate azimuth and elevation through multitask learning, which generates the posterior probability for candidate directions. Finally, the candidate direction with the highest probability is taken as the final direction estimation. The experiments based on public head-related transfer function (HRTF) database demonstrate that the proposed method achieves preferable localization performance compared with other popular methods.},
  urldate  = {2024-06-07},
  journal  = {IEEE Access},
  author   = {Pang, Cheng and Liu, Hong and Li, Xiaofei},
  year     = {2019},
  note     = {Conference Name: IEEE Access},
  keywords = {Acoustics, Azimuth, convolutional neural network, Estimation, Feature extraction, multitask learning, Neural networks, Noise measurement, Sound source localization, time-frequency, Time-frequency analysis},
  pages    = {40725--40737}
}


@article{arthi_spatiogram_2021,
  title        = {Spatiogram: A phase based directional angular measure and perceptual weighting for ensemble source width},
  volume       = {abs/2112.07216},
  url          = {https://api.semanticscholar.org/CorpusID:245131510},
  journaltitle = {{ArXiv}},
  author       = {Arthi, S. and Sreenivas, Thippur V.},
  date         = {2021},
  file         = {arXiv Fulltext PDF:/home/pawel/Zotero/storage/IRU2Y2LJ/S and T V - 2021 - Spatiogram A phase based directional angular meas.pdf:application/pdf}
}


# speech


@article{wang_binaural_2020,
  title        = {Binaural sound localization based on deep neural network and affinity propagation clustering in mismatched {HRTF} condition},
  volume       = {2020},
  issn         = {1687-4722},
  url          = {https://doi.org/10.1186/s13636-020-0171-y},
  doi          = {10.1186/s13636-020-0171-y},
  abstract     = {Binaural sound source localization is an important and widely used perceptually based method and it has been applied to machine learning studies by many researchers based on head-related transfer function ({HRTF}). Because the {HRTF} is closely related to human physiological structure, the {HRTFs} vary between individuals. Related machine learning studies to date tend to focus on binaural localization in reverberant or noisy environments, or in conditions with multiple simultaneously active sound sources. In contrast, mismatched {HRTF} condition, in which the {HRTFs} used to generate the training and test sets are different, is rarely studied. This mismatch leads to a degradation of localization performance. A basic solution to this problem is to introduce more data to improve generalization performance, which requires a lot. However, simply increasing the data volume will result in data-inefficiency. In this paper, we propose a data-efficient method based on deep neural network ({DNN}) and clustering to improve binaural localization performance in the mismatched {HRTF} condition. Firstly, we analyze the relationship between binaural cues and the sound source localization with a classification {DNN}. Different {HRTFs} are used to generate training and test sets, respectively. On this basis, we study the localization performance of {DNN} model trained by each training set on different test sets. The result shows that the localization performance of the same model on different test sets is different, while the localization performance of different models on the same test set may be similar. The result also shows a clustering trend. Secondly, different {HRTFs} are divided into several clusters. Finally, the corresponding {HRTFs} of each cluster center are selected to generate a new training set and to train a more generalized {DNN} model. The experimental results show that the proposed method achieves better generalization performance than the baseline methods in the mismatched {HRTF} condition and has almost equal performance to the {DNN} trained with a large number of {HRTFs}, which means the proposed method is data-efficient.},
  pages        = {4},
  number       = {1},
  journaltitle = {{EURASIP} Journal on Audio, Speech, and Music Processing},
  shortjournal = {{EURASIP} Journal on Audio, Speech, and Music Processing},
  author       = {Wang, Jing and Wang, Jin and Qian, Kai and Xie, Xiang and Kuang, Jingming},
  urldate      = {2024-06-07},
  date         = {2020-02-10},
  keywords     = {Affinity propagation, Binaural localization, Clustering, Deep neural network},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/AXJ7NQJI/Wang et al. - 2020 - Binaural sound localization based on deep neural n.pdf:application/pdf;Snapshot:/Users/pawel/Zotero/storage/HXP2Q4IJ/s13636-020-0171-y.html:text/html}
}


@article{liu_multiple_2018,
  title        = {Multiple Speaker Tracking in Spatial Audio via {PHD} Filtering and Depth-Audio Fusion},
  volume       = {20},
  issn         = {1941-0077},
  url          = {https://ieeexplore.ieee.org/document/8119824},
  doi          = {10.1109/TMM.2017.2777671},
  abstract     = {In the object-based spatial audio system, positions of the audio objects (e.g., speakers/talkers or voices) presented in the sound scene are required as important metadata attributes for object acquisition and reproduction. Binaural microphones are often used as a physical device to mimic human hearing and to monitor and analyze the scene, including localization and tracking of multiple speakers. The binaural audio tracker, however, is usually prone to the errors caused by room reverberation and background noise. To address this limitation, we present a multimodal tracking method by fusing the binaural audio with depth information (from a depth sensor, e.g., Kinect). More specifically, the probability hypothesis density ({PHD}) filtering framework is first applied to the depth stream, and a novel clutter intensity model is proposed to improve the robustness of the {PHD} filter when an object is occluded either by other objects or due to the limited field of view of the depth sensor. To compensate misdetections in the depth stream, a novel gap filling technique is presented to map audio azimuths obtained from the binaural audio tracker to 3D positions, using speaker-dependent spatial constraints learned from the depth stream. With our proposed method, both the errors in the binaural tracker and the misdetections in the depth tracker can be significantly reduced. Real-room recordings are used to show the improved performance of the proposed method in removing outliers and reducing misdetections.},
  pages        = {1767--1780},
  number       = {7},
  journaltitle = {{IEEE} Transactions on Multimedia},
  author       = {Liu, Qingju and Wang, Wenwu and de Campos, Teófilo and Jackson, Philip J. B. and Hilton, Adrian},
  urldate      = {2024-06-07},
  date         = {2018-07},
  note         = {Conference Name: {IEEE} Transactions on Multimedia},
  keywords     = {Azimuth, binaural microphones, Clutter, depth and audio, depth sensor, Metadata, Microphones, Multi-person tracking, {PHD} filtering, spatial audio, Target tracking, Three-dimensional displays, Trajectory},
  file         = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/ETBEG7F9/8119824.html:text/html}
}


@article{ma_robust_2018,
  title        = {Robust Binaural Localization of a Target Sound Source by Combining Spectral Source Models and Deep Neural Networks},
  volume       = {26},
  issn         = {2329-9304},
  url          = {https://ieeexplore.ieee.org/document/8410799},
  doi          = {10.1109/TASLP.2018.2855960},
  abstract     = {Despite there being a clear evidence for top-down (e.g., attentional) effects in biological spatial hearing, relatively few machine hearing systems exploit the top-down model-based knowledge in sound localization. This paper addresses this issue by proposing a novel framework for the binaural sound localization that combines the model-based information about the spectral characteristics of sound sources and deep neural networks ({DNNs}). A target source model and a background source model are first estimated during a training phase using spectral features extracted from sound signals in isolation. When the identity of the background source is not available, a universal background model can be used. During testing, the source models are used jointly to explain the mixed observations and improve the localization process by selectively weighting source azimuth posteriors output by a {DNN}-based localization system. To address the possible mismatch between the training and testing, a model adaptation process is further employed the on-the-fly during testing, which adapts the background model parameters directly from the noisy observations in an iterative manner. The proposed system, therefore, combines the model-based and data-driven information flow within a single computational framework. The evaluation task involved localization of a target speech source in the presence of an interfering source and room reverberation. Our experiments show that by exploiting the model-based information in this way, the sound localization performance can be improved substantially under various noisy and reverberant conditions.},
  pages        = {2122--2131},
  number       = {11},
  journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  author       = {Ma, Ning and Gonzalez, Jose A. and Brown, Guy J.},
  urldate      = {2024-06-07},
  date         = {2018-11},
  note         = {Conference Name: {IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
  keywords     = {Adaptation models, Auditory system, Azimuth, Binaural source localisation, Biological system modeling, Computational modeling, machine hearing, masking, reverberation, sound source combination, Speech processing, Time-frequency analysis},
  file         = {Accepted Version:/Users/pawel/Zotero/storage/LZQN43AY/Ma et al. - 2018 - Robust Binaural Localization of a Target Sound Sou.pdf:application/pdf}
}

# our


@inproceedings{antoniuk_blind_2023,
  title     = {Blind estimation of ensemble width in binaural music recordings using ‘spatiograms’ under simulated anechoic conditions},
  rights    = {All rights reserved},
  url       = {http://www.aes.org/e-lib/browse.cfm?elib=22203},
  booktitle = {Audio Engineering Society Conference: {AES} 2023 International Conference on Spatial and Immersive Audio},
  author    = {Antoniuk, Paweł and Zieliński, Sławomir K.},
  date      = {2023-08}
}


@article{zielinski_automatic_2022,
  title        = {Automatic discrimination between front and back ensemble locations in {HRTF}-convolved binaural recordings of music},
  volume       = {2022},
  issn         = {1687-4722},
  url          = {https://doi.org/10.1186/s13636-021-00235-2},
  doi          = {10.1186/s13636-021-00235-2},
  abstract     = {One of the greatest challenges in the development of binaural machine audition systems is the disambiguation between front and back audio sources, particularly in complex spatial audio scenes. The goal of this work was to develop a method for discriminating between front and back located ensembles in binaural recordings of music. To this end, 22, 496 binaural excerpts, representing either front or back located ensembles, were synthesized by convolving multi-track music recordings with 74 sets of head-related transfer functions ({HRTF}). The discrimination method was developed based on the traditional approach, involving hand-engineering of features, as well as using a deep learning technique incorporating the convolutional neural network ({CNN}). According to the results obtained under {HRTF}-dependent test conditions, {CNN} showed a very high discrimination accuracy (99.4\%), slightly outperforming the traditional method. However, under the {HRTF}-independent test scenario, {CNN} performed worse than the traditional algorithm, highlighting the importance of testing the algorithms under {HRTF}-independent conditions and indicating that the traditional method might be more generalizable than {CNN}. A minimum of 20 {HRTFs} are required to achieve a satisfactory generalization performance for the traditional algorithm and 30 {HRTFs} for {CNN}. The minimum duration of audio excerpts required by both the traditional and {CNN}-based methods was assessed as 3 s. Feature importance analysis, based on a gradient attribution mapping technique, revealed that for both the traditional and the deep learning methods, a frequency band between 5 and 6 {kHz} is particularly important in terms of the discrimination between front and back ensemble locations. Linear-frequency cepstral coefficients, interaural level differences, and audio bandwidth were identified as the key descriptors facilitating the discrimination process using the traditional approach.},
  pages        = {3},
  number       = {1},
  journaltitle = {{EURASIP} Journal on Audio, Speech, and Music Processing},
  shortjournal = {{EURASIP} Journal on Audio, Speech, and Music Processing},
  author       = {Zieliński, Sławomir K. and Antoniuk, Paweł and Lee, Hyunkook and Johnson, Dale},
  urldate      = {2024-06-10},
  date         = {2022-01-15},
  keywords     = {Binaural recordings, Feature engineering, {HRTF}, Spatial audio information retrieval},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/4G5WL8AM/Zieliński et al. - 2022 - Automatic discrimination between front and back en.pdf:application/pdf;Snapshot:/Users/pawel/Zotero/storage/SI2K4EJ6/s13636-021-00235-2.html:text/html}
}


@article{zielinski_spatial_2022,
  title        = {Spatial Audio Scene Characterization ({SASC}): Automatic Localization of Front-, Back-, Up-, and Down-Positioned Music Ensembles in Binaural Recordings},
  volume       = {12},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2076-3417},
  url          = {https://www.mdpi.com/2076-3417/12/3/1569},
  doi          = {10.3390/app12031569},
  shorttitle   = {Spatial Audio Scene Characterization ({SASC})},
  abstract     = {The automatic localization of audio sources distributed symmetrically with respect to coronal or transverse planes using binaural signals still poses a challenging task, due to the front–back and up–down confusion effects. This paper demonstrates that the convolutional neural network ({CNN}) can be used to automatically localize music ensembles panned to the front, back, up, or down positions. The network was developed using the repository of the binaural excerpts obtained by the convolution of multi-track music recordings with the selected sets of head-related transfer functions ({HRTFs}). They were generated in such a way that a music ensemble (of circular shape in terms of its boundaries) was positioned in one of the following four locations with respect to the listener: front, back, up, and down. According to the obtained results, {CNN} identified the location of the ensembles with the average accuracy levels of 90.7\% and 71.4\% when tested under the {HRTF}-dependent and {HRTF}-independent conditions, respectively. For {HRTF}-dependent tests, the accuracy decreased monotonically with the increase in the ensemble size. A modified image occlusion sensitivity technique revealed selected frequency bands as being particularly important in terms of the localization process. These frequency bands are largely in accordance with the psychoacoustical literature.},
  pages        = {1569},
  number       = {3},
  journaltitle = {Applied Sciences},
  author       = {Zieliński, Sławomir K. and Antoniuk, Paweł and Lee, Hyunkook},
  urldate      = {2024-06-10},
  date         = {2022-01},
  langid       = {english},
  note         = {Number: 3
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {convolutional neural networks, deep learning, spatial audio information retrieval, spatial audio scene characterization},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/HDGFITDN/Zieliński et al. - 2022 - Spatial Audio Scene Characterization (SASC) Autom.pdf:application/pdf}
}


@article{zielinski_comparison_2020,
  title        = {A Comparison of Human against Machine-Classification of Spatial Audio Scenes in Binaural Recordings of Music},
  volume       = {10},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2076-3417},
  url          = {https://www.mdpi.com/2076-3417/10/17/5956},
  doi          = {10.3390/app10175956},
  abstract     = {The purpose of this paper is to compare the performance of human listeners against the selected machine learning algorithms in the task of the classification of spatial audio scenes in binaural recordings of music under practical conditions. The three scenes were subject to classification: (1) music ensemble (a group of musical sources) located in the front, (2) music ensemble located at the back, and (3) music ensemble distributed around a listener. In the listening test, undertaken remotely over the Internet, human listeners reached the classification accuracy of 42.5\%. For the listeners who passed the post-screening test, the accuracy was greater, approaching 60\%. The above classification task was also undertaken automatically using four machine learning algorithms: convolutional neural network, support vector machines, extreme gradient boosting framework, and logistic regression. The machine learning algorithms substantially outperformed human listeners, with the classification accuracy reaching 84\%, when tested under the binaural-room-impulse-response ({BRIR}) matched conditions. However, when the algorithms were tested under the {BRIR} mismatched scenario, the accuracy obtained by the algorithms was comparable to that exhibited by the listeners who passed the post-screening test, implying that the machine learning algorithms capability to perform in unknown electro-acoustic conditions needs to be further improved.},
  pages        = {5956},
  number       = {17},
  journaltitle = {Applied Sciences},
  author       = {Zieliński, Sławomir K. and Lee, Hyunkook and Antoniuk, Paweł and Dadan, Oskar},
  urldate      = {2024-06-10},
  date         = {2020-01},
  langid       = {english},
  note         = {Number: 17
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {convolutional neural networks, deep learning, spatial audio information retrieval, spatial audio scene classification},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/SC5RHNYP/Zieliński et al. - 2020 - A Comparison of Human against Machine-Classificati.pdf:application/pdf}
}


# glass-box methods

# black-box methods



# corpus


@inproceedings{garofolo_darpa_1993,
  title  = {{DARPA} {TIMIT}:: acoustic-phonetic continuous speech corpus {CD}-{ROM}, {NIST} speech disc 1-1.1},
  url    = {https://api.semanticscholar.org/CorpusID:60884624},
  author = {Garofolo, John S. and Lamel, Lori and Fisher, William M. and Fiscus, Jonathan G. and Pallett, David S. and Dahlgren, Nancy L.},
  date   = {1993}
}



@online{senior_mixing_2023,
  title   = {The 'Mixing Secrets' Free Multitrack Download Library},
  rights  = {Cambridge Music Technology},
  url     = {https://cambridge-mt.com/ms/mtk/},
  author  = {Senior, Mike},
  urldate = {2024-06-10},
  date    = {2023},
  note    = {Music recording repository},
  file    = {The 'Mixing Secrets' Free Multitrack Download Library:/home/pawel/Zotero/storage/C6X26KMY/mtk.html:text/html}
}


@online{raake_computational_nodate,
  title   = {A computational framework for modelling active exploratory listening that assigns meaning to auditory scenes—reading the world with two ears},
  url     = {http://twoears.eu/},
  author  = {Raake, Alexander},
  urldate = {2024-06-11},
  file    = {Two!Ears - Project:/Users/pawel/Zotero/storage/ZNPGKEQD/project.html:text/html}
}

@inproceedings{lecun_handwritten_1989,
  title     = {Handwritten Digit Recognition with a Back-Propagation Network},
  volume    = {2},
  url       = {https://proceedings.neurips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html},
  abstract  = {We present an application of back-propagation networks to hand(cid:173) written digit recognition. Minimal preprocessing of the data was  required, but architecture of the network was highly constrained  and specifically designed for the task. The input of the network  consists of normalized images of isolated digits. The method has  1 \% error rate and about a 9\% reject rate on zipcode digits provided  by the U.S. Postal Service.},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Morgan-Kaufmann},
  author    = {{LeCun}, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  urldate   = {2024-06-14},
  date      = {1989},
  file      = {Full Text PDF:/Users/pawel/Zotero/storage/LQ2N3GLG/LeCun et al. - 1989 - Handwritten Digit Recognition with a Back-Propagat.pdf:application/pdf}
}


@article{lecun_deep_2015,
  title        = {Deep learning},
  volume       = {521},
  issn         = {0028-0836, 1476-4687},
  url          = {https://www.nature.com/articles/nature14539},
  doi          = {10.1038/nature14539},
  pages        = {436--444},
  number       = {7553},
  journaltitle = {Nature},
  shortjournal = {Nature},
  author       = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  urldate      = {2024-06-14},
  date         = {2015-05-28},
  langid       = {english},
  file         = {Submitted Version:/Users/pawel/Zotero/storage/E78H3576/LeCun et al. - 2015 - Deep learning.pdf:application/pdf}
}

% CNN internals

@inproceedings{krizhevsky_imagenet_2012,
  title     = {{ImageNet} Classification with Deep Convolutional Neural Networks},
  volume    = {25},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  urldate   = {2024-06-19},
  date      = {2012},
  file      = {Full Text PDF:/Users/pawel/Zotero/storage/NKTH38ND/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf}
}



@article{srivastava_dropout_2014,
  title        = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  volume       = {15},
  url          = {http://jmlr.org/papers/v15/srivastava14a.html},
  pages        = {1929--1958},
  number       = {56},
  journaltitle = {Journal of Machine Learning Research},
  author       = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date         = {2014}
}

@inproceedings{ioffe_batch_2015,
  location  = {Lille, France},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  volume    = {37},
  url       = {https://proceedings.mlr.press/v37/ioffe15.html},
  series    = {Proceedings of Machine Learning Research},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
  pages     = {448--456},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  publisher = {{PMLR}},
  author    = {Ioffe, Sergey and Szegedy, Christian},
  editor    = {Bach, Francis and Blei, David},
  date      = {2015-07-07}
}



@article{lin_network_2013,
  title        = {Network In Network},
  volume       = {abs/1312.4400},
  url          = {https://api.semanticscholar.org/CorpusID:16636683},
  journaltitle = {{CoRR}},
  author       = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  date         = {2013},
  file         = {arXiv Fulltext PDF:/home/pawel/Zotero/storage/3FFXZK4D/Lin et al. - 2014 - Network In Network.pdf:application/pdf}
}


% Datasets


@inproceedings{braren_high-resolution_2020,
  title  = {A High-Resolution Individual 3D Adult Head and Torso Model for {HRTF} Simulation and Validation: {HRTF} Measurement},
  url    = {https://api.semanticscholar.org/CorpusID:234998299},
  author = {Braren, Hark Simon and Fels, Janina},
  date   = {2020},
  file   = {Braren and Fels - 2020 - A High-Resolution Individual 3D Adult Head and Tor.pdf:/home/pawel/Zotero/storage/KKR3ZBZ8/Braren and Fels - 2020 - A High-Resolution Individual 3D Adult Head and Tor.pdf:application/pdf}
}

@online{noauthor_hrtf-database_nodate,
  title      = {{HRTF}-Database},
  url        = {https://www.oeaw.ac.at/en/ari/das-institut/software/hrtf-database},
  titleaddon = {Austrian Academy of Sciences},
  type       = {Acoustic Research Institute},
  urldate    = {2024-06-19},
  file       = {HRTF-Database:/Users/pawel/Zotero/storage/F6K6MNKB/hrtf-database.html:text/html}
}


@inproceedings{algazi_cipic_2001,
  title      = {The {CIPIC} {HRTF} database},
  url        = {https://ieeexplore.ieee.org/document/969552},
  doi        = {10.1109/ASPAA.2001.969552},
  abstract   = {This paper describes a public-domain database of high-spatial-resolution head-related transfer functions measured at the {UC} Davis {CIPIC} Interface Laboratory and the methods used to collect the data.. Release 1.0 (see http://interface.cipic.ucdavis.edu) includes head-related impulse responses for 45 subjects at 25 different azimuths and 50 different elevations (1250 directions) at approximately 5/spl deg/ angular increments. In addition, the database contains anthropometric measurements for each subject. Statistics of anthropometric parameters and correlations between anthropometry and some temporal and spectral features of the {HRTFs} are reported.},
  eventtitle = {Proceedings of the 2001 {IEEE} Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No.01TH8575)},
  pages      = {99--102},
  booktitle  = {Proceedings of the 2001 {IEEE} Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No.01TH8575)},
  author     = {Algazi, V.R. and Duda, R.O. and Thompson, D.M. and Avendano, C.},
  urldate    = {2024-06-19},
  date       = {2001-10},
  keywords   = {Acoustic scattering, Anthropometry, Azimuth, Ear, Laboratories, Microphones, Position measurement, Spatial databases, Statistics, Transfer functions},
  file       = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/8YD4T6PV/969552.html:text/html}
}


@article{andreopoulou_inter-laboratory_2015,
  title        = {Inter-Laboratory Round Robin {HRTF} Measurement Comparison},
  volume       = {9},
  issn         = {1941-0484},
  url          = {https://ieeexplore.ieee.org/document/7031925},
  doi          = {10.1109/JSTSP.2015.2400417},
  abstract     = {Head-Related Transfer Function ({HRTF}) measurements underlie the signal processing used in binaural auditory displays, but measurement techniques, equipment, and post-processing vary substantially between laboratories. This variation can result in significant differences in measured spectral and timing data taken from the same subject for the same sound source locations. An ongoing project for comparing databases from laboratories across the world (colloquially titled “Club Fritz”) employs a single dummy head microphone for measurements (Neumann {KU}-100) at various sites. The current study examines magnitude and timing differences between left and right ear data from 12 different {HRTF} sets taken from 10 different laboratories. Results revealed spectral magnitude variations up to 12.5 {dB} for frequency bands below 6 {kHz} and up to 23 {dB} above that, as well as large spectral left/right asymmetries (dcorr ≤ 0.4) for high-frequency content. Further subjective studies are necessary to determine the perceptual relevance of these findings. Nevertheless, the observed {ITD} variations of up to 235 μsec are alarming as they often exceeded reported {JND} values. Such findings highlight the potential impact of physical spaces, measurement routines, and equipment types on the collected {HRTF} data.},
  pages        = {895--906},
  number       = {5},
  journaltitle = {{IEEE} Journal of Selected Topics in Signal Processing},
  author       = {Andreopoulou, Areti and Begault, Durand R. and Katz, Brian F. G.},
  urldate      = {2024-06-19},
  date         = {2015-08},
  note         = {Conference Name: {IEEE} Journal of Selected Topics in Signal Processing},
  keywords     = {Current measurement, Databases, Ear, {HRTF}/{HRIR}, {ITD} variations, Laboratories, measurement, Microphones, Position measurement, repeatability, Spatial resolution, spatial symmetry, spectral variations},
  file         = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/S29TAY72/7031925.html:text/html}
}


@article{brinkmann_cross-evaluated_2019,
  title        = {A Cross-Evaluated Database of Measured and Simulated {HRTFs} Including 3D Head Meshes, Anthropometric Features, and Headphone Impulse Responses},
  volume       = {67},
  issn         = {15494950},
  url          = {http://www.aes.org/e-lib/browse.cfm?elib=20546},
  doi          = {10.17743/jaes.2019.0024},
  pages        = {705--718},
  number       = {9},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  author       = {Brinkmann, Fabian and Dinakaran, Manoj and Pelzer, Robert and Grosche, Peter and Voss, Daniel and Weinzierl, Stefan},
  urldate      = {2024-06-19},
  date         = {2019-09-21},
  langid       = {english},
  file         = {Brinkmann et al. - 2019 - A Cross-Evaluated Database of Measured and Simulat.pdf:/Users/pawel/Zotero/storage/8RIN3L7T/Brinkmann et al. - 2019 - A Cross-Evaluated Database of Measured and Simulat.pdf:application/pdf}
}



@inproceedings{spagnol_viking_2019,
  title  = {{THE} {VIKING} {HRTF} {DATASET}},
  url    = {https://api.semanticscholar.org/CorpusID:249669523},
  author = {Spagnol, Simone},
  date   = {2019}
}



@inproceedings{porschmann_spherical_2017,
  title    = {A Spherical Near-Field {HRTF} Set for Auralization and Psychoacoustic Research},
  abstract = {Head-related transfer functions ({HRTFs}) describe the directional filtering caused by the head, pinna, and torso and are an essential component of binaural synthesis systems. Currently most of these systems are based on far-field {HRTFs} and thus do not consider acoustical specifics of nearby sound sources. One reason might be that full spherical near-field {HRTF} sets are rarely available. In this paper we present an {HRTF} set of a Neumann {KU}100 dummy head and a technical evaluation of the set. The set is freely available for download and contains post-processed impulse responses, captured on a circular and full spherical grid at distances between 0.25 m and 1.50 m. It can be used for psychoacoustic research and for applications where nearby virtual sound sources shall be auralized.
              
              Engineering Brief 322
              http://www.aes.org/e-lib/browse.cfm?elib=18697},
  author   = {Pörschmann, Christoph and Arend, Johannes and Neidhardt, Annika},
  date     = {2017-05-21}
}

@article{wierstorf_free_2011,
  title    = {A Free Database of Head-Related Impulse Response Measurements in the Horizontal Plane with Multiple Distances},
  abstract = {A freely available collection of Head-Related Impulse Response ({HRIR}) measurements is introduced. The impulse responses were acquired in an anechoic chamber using a {KEMAR} manikin at four diﬀerent loudspeaker distances – 3 m, 2 m, 1 m and 0.5 m – reaching from the far ﬁeld to the near ﬁeld. The loudspeaker was positioned at ear height and the manikin was rotated with a high-precision stepper motor in one degree increments. Besides the raw {HRIRs} also datasets are available which have been compensated for the use with speciﬁc headphone models.},
  author   = {Wierstorf, Hagen and Geier, Matthias and Raake, Alexander and Spors, Sascha},
  date     = {2011},
  langid   = {english},
  file     = {Wierstorf et al. - 2011 - A Free Database of Head-Related Impulse Response M.pdf:/Users/pawel/Zotero/storage/A66C4WXR/Wierstorf et al. - 2011 - A Free Database of Head-Related Impulse Response M.pdf:application/pdf}
}


@inproceedings{spagnol_viking_2020,
  title  = {The Viking {HRTF} dataset v2},
  url    = {https://api.semanticscholar.org/CorpusID:244994009},
  author = {Spagnol, Simone and Miccini, Riccardo and Unnthorsson, Runar},
  date   = {2020}
}



@online{noauthor_listen_2023,
  title   = {{LISTEN} {HRTF} {DATABASE}},
  url     = {http://recherche.ircam.fr/equipes/salles/listen/},
  urldate = {2024-06-19},
  date    = {2023},
  file    = {LISTEN HRTF DATABASE:/Users/pawel/Zotero/storage/66K5BKAI/listen.html:text/html}
}


@online{gardne_hrtf_1994,
  title   = {{HRTF} Measurements of a {KEMAR} Dummy-Head Microphone},
  rights  = {{MIT} Media Lab},
  url     = {https://sound.media.mit.edu/resources/KEMAR.html},
  author  = {Gardne, Bill and Martin, Keith},
  urldate = {2024-06-19},
  date    = {1994},
  file    = {HRTF Measurements of a KEMAR Dummy-Head Microphone:/Users/pawel/Zotero/storage/4HFJMLXI/KEMAR.html:text/html}
}


@article{watanabe_dataset_2014,
  title        = {Dataset of head-related transfer functions measured with a circular loudspeaker array},
  volume       = {35},
  issn         = {1346-3969, 1347-5177},
  url          = {https://www.jstage.jst.go.jp/article/ast/35/3/35_E1368/_article},
  doi          = {10.1250/ast.35.159},
  abstract     = {In this paper, we describe a dataset of head-related transfer functions ({HRTFs}) measured at the Research Institute of Electrical Communications, Tohoku University. The current dataset includes {HRTFs} for 105 subjects at 72 azimuths Â 13 elevations of spherical coordinates. Anthropometric data for 39 subjects are also included. The measurement and postprocessing methods are outlined in this paper. These data will be freely accessible for nonproﬁt academic purposes via the Internet. Moreover, this dataset will be included in an international joint project to gather several {HRTF} datasets in a uniﬁed data format.},
  pages        = {159--165},
  number       = {3},
  journaltitle = {Acoustical Science and Technology},
  shortjournal = {Acoust. Sci. \& Tech.},
  author       = {Watanabe, Kanji and Iwaya, Yukio and Suzuki, Yôiti and Takane, Shouichi and Sato, Sojun},
  urldate      = {2024-06-19},
  date         = {2014},
  langid       = {english},
  file         = {Watanabe et al. - 2014 - Dataset of head-related transfer functions measure.pdf:/Users/pawel/Zotero/storage/5TRRD52B/Watanabe et al. - 2014 - Dataset of head-related transfer functions measure.pdf:application/pdf}
}


@article{armstrong_perceptual_2018,
  title        = {A Perceptual Evaluation of Individual and Non-Individual {HRTFs}: A Case Study of the {SADIE} {II} Database},
  volume       = {8},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {2076-3417},
  url          = {https://www.mdpi.com/2076-3417/8/11/2029},
  doi          = {10.3390/app8112029},
  shorttitle   = {A Perceptual Evaluation of Individual and Non-Individual {HRTFs}},
  abstract     = {As binaural audio continues to permeate immersive technologies, it is vital to develop a detailed understanding of the perceptual relevance of {HRTFs}. Previous research has explored the benefit of individual {HRTFs} with respect to localisation. However, localisation is only one metric with which it is possible to rate spatial audio. This paper evaluates the perceived timbral and spatial characteristics of both individual and non-individual {HRTFs} and compares the results to overall preference. To that end, the measurement and evaluation of a high-resolution multi-environment binaural Impulse Response database is presented for 20 subjects, including the {KU}100 and {KEMAR} binaural mannequins. Post-processing techniques, including low frequency compensation and diffuse field equalisation are discussed in relation to the 8802 unique {HRTFs} measured for each mannequin and 2818/2114 {HRTFs} measured for each human. Listening test results indicate that particular {HRTF} sets are preferred more generally by subjects over their own individual measurements.},
  pages        = {2029},
  number       = {11},
  journaltitle = {Applied Sciences},
  author       = {Armstrong, Cal and Thresh, Lewis and Murphy, Damian and Kearney, Gavin},
  urldate      = {2024-06-19},
  date         = {2018-11},
  langid       = {english},
  note         = {Number: 11
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {binaural, database, evaluation, {HRTF}, measurement, perception, spatial audio, timbre},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/JUFP5BZ5/Armstrong et al. - 2018 - A Perceptual Evaluation of Individual and Non-Indi.pdf:application/pdf}
}


@article{yu_near-field_2018,
  title        = {Near-field head-related transfer-function measurement and database of human subjects},
  volume       = {143},
  issn         = {0001-4966, 1520-8524},
  url          = {https://pubs.aip.org/jasa/article/143/3/EL194/609761/Near-field-head-related-transfer-function},
  doi          = {10.1121/1.5027019},
  abstract     = {Near-ﬁeld head-related transfer functions ({HRTFs}) of human subjects are essential to those researching spatial hearing. By using a carefully designed measurement system, near-ﬁeld {HRTFs} of human subjects were measured and a database was constructed. The database includes 56 Chinese human subjects, seven source distances from 0.2 to 1.0 m, and 685 directions at each distance for each subject. In the present work, the technique of near-ﬁeld {HRTF} measurement is outlined, the performance of the measurement system is assessed and validated, and the resultant database is reported. The database can provide fundamental data for future research.},
  pages        = {EL194--EL198},
  number       = {3},
  journaltitle = {The Journal of the Acoustical Society of America},
  author       = {Yu, Guangzheng and Wu, Ruixing and Liu, Yu and Xie, Bosun},
  urldate      = {2024-06-19},
  date         = {2018-03-01},
  langid       = {english},
  file         = {Yu et al. - 2018 - Near-field head-related transfer-function measurem.pdf:/Users/pawel/Zotero/storage/U66CIAPL/Yu et al. - 2018 - Near-field head-related transfer-function measurem.pdf:application/pdf}
}


@inproceedings{porschmann_spherical_2017,
  title    = {A Spherical Near-Field {HRTF} Set for Auralization and Psychoacoustic Research},
  abstract = {Head-related transfer functions ({HRTFs}) describe the directional filtering caused by the head, pinna, and torso and are an essential component of binaural synthesis systems. Currently most of these systems are based on far-field {HRTFs} and thus do not consider acoustical specifics of nearby sound sources. One reason might be that full spherical near-field {HRTF} sets are rarely available. In this paper we present an {HRTF} set of a Neumann {KU}100 dummy head and a technical evaluation of the set. The set is freely available for download and contains post-processed impulse responses, captured on a circular and full spherical grid at distances between 0.25 m and 1.50 m. It can be used for psychoacoustic research and for applications where nearby virtual sound sources shall be auralized.
              
              Engineering Brief 322
              http://www.aes.org/e-lib/browse.cfm?elib=18697},
  author   = {Pörschmann, Christoph and Arend, Johannes and Neidhardt, Annika},
  date     = {2017-05-21}
}


@article{yu_near-field_2018,
  title        = {Near-field head-related transfer-function measurement and database of human subjects},
  volume       = {143},
  issn         = {0001-4966, 1520-8524},
  url          = {https://pubs.aip.org/jasa/article/143/3/EL194/609761/Near-field-head-related-transfer-function},
  doi          = {10.1121/1.5027019},
  abstract     = {Near-ﬁeld head-related transfer functions ({HRTFs}) of human subjects are essential to those researching spatial hearing. By using a carefully designed measurement system, near-ﬁeld {HRTFs} of human subjects were measured and a database was constructed. The database includes 56 Chinese human subjects, seven source distances from 0.2 to 1.0 m, and 685 directions at each distance for each subject. In the present work, the technique of near-ﬁeld {HRTF} measurement is outlined, the performance of the measurement system is assessed and validated, and the resultant database is reported. The database can provide fundamental data for future research.},
  pages        = {EL194--EL198},
  number       = {3},
  journaltitle = {The Journal of the Acoustical Society of America},
  author       = {Yu, Guangzheng and Wu, Ruixing and Liu, Yu and Xie, Bosun},
  urldate      = {2024-06-19},
  date         = {2018-03-01},
  langid       = {english},
  file         = {Yu et al. - 2018 - Near-field head-related transfer-function measurem.pdf:/Users/pawel/Zotero/storage/U66CIAPL/Yu et al. - 2018 - Near-field head-related transfer-function measurem.pdf:application/pdf}
}


@article{brinkmann_high_2017,
  title        = {A High Resolution and Full-Spherical Head-Related Transfer Function Database for Different Head-Above-Torso Orientations},
  volume       = {65},
  issn         = {15494950},
  url          = {http://www.aes.org/e-lib/browse.cfm?elib=19357},
  doi          = {10.17743/jaes.2017.0033},
  pages        = {841--848},
  number       = {10},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  author       = {Brinkmann, Fabian and Lindau, Alexander and Weinzerl, Stefan and Van De Par, Steven and Müller-Trapet, Markus and Opdam, Rob and Vorländer, Michael},
  urldate      = {2024-06-19},
  date         = {2017-10-30},
  langid       = {english},
  file         = {Brinkmann et al. - 2017 - A High Resolution and Full-Spherical Head-Related .pdf:/Users/pawel/Zotero/storage/GZFNE9CA/Brinkmann et al. - 2017 - A High Resolution and Full-Spherical Head-Related .pdf:application/pdf}
}




% CNN in audio w/ spectrograms

@inproceedings{thomas_analyzing_2014,
  location   = {Florence, Italy},
  title      = {Analyzing convolutional neural networks for speech activity detection in mismatched acoustic conditions},
  isbn       = {978-1-4799-2893-4},
  url        = {http://ieeexplore.ieee.org/document/6854054/},
  doi        = {10.1109/ICASSP.2014.6854054},
  abstract   = {Convolutional neural networks ({CNN}) are extensions to deep neural networks ({DNN}) which are used as alternate acoustic models with state-of-the-art performances for speech recognition. In this paper, {CNNs} are used as acoustic models for speech activity detection ({SAD}) on data collected over noisy radio communication channels. When these {SAD} models are tested on audio recorded from radio channels not seen during training, there is severe performance degradation. We attribute this degradation to mismatches between the two dimensional ﬁlters learnt in the initial {CNN} layers and the novel channel data. Using a small amount of supervised data from the novel channels, the ﬁlters can be adapted to provide signiﬁcant improvements in {SAD} performance. In mismatched acoustic conditions, the adapted models provide signiﬁcant improvements (about 10-25\%) relative to conventional {DNN}-based {SAD} systems. These results illustrate that {CNNs} have a considerable advantage in fast adaptation for acoustic modeling in these settings.},
  eventtitle = {{ICASSP} 2014 - 2014 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  pages      = {2519--2523},
  booktitle  = {2014 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  publisher  = {{IEEE}},
  author     = {Thomas, Samuel and Ganapathy, Sriram and Saon, George and Soltau, Hagen},
  urldate    = {2024-06-19},
  date       = {2014-05},
  langid     = {english},
  file       = {Thomas et al. - 2014 - Analyzing convolutional neural networks for speech.pdf:/Users/pawel/Zotero/storage/RAIR3LKP/Thomas et al. - 2014 - Analyzing convolutional neural networks for speech.pdf:application/pdf}
}


@article{espi_exploiting_2015,
  title        = {Exploiting spectro-temporal locality in deep learning based acoustic event detection},
  volume       = {2015},
  issn         = {1687-4722},
  url          = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0069-2},
  doi          = {10.1186/s13636-015-0069-2},
  abstract     = {In recent years, deep learning has not only permeated the computer vision and speech recognition research fields but also fields such as acoustic event detection ({AED}). One of the aims of {AED} is to detect and classify non-speech acoustic events occurring in conversation scenes including those produced by both humans and the objects that surround us. In {AED}, deep learning has enabled modeling of detail-rich features, and among these, high resolution spectrograms have shown a significant advantage over existing predefined features (e.g., Mel-filter bank) that compress and reduce detail. In this paper, we further asses the importance of feature extraction for deep learning-based acoustic event detection. {AED}, based on spectrogram-input deep neural networks, exploits the fact that sounds have “global” spectral patterns, but sounds also have “local” properties such as being more transient or smoother in the time-frequency domain. These can be exposed by adjusting the time-frequency resolution used to compute the spectrogram, or by using a model that exploits locality leading us to explore two different feature extraction strategies in the context of deep learning: (1) using multiple resolution spectrograms simultaneously and analyzing the overall and event-wise influence to combine the results, and (2) introducing the use of convolutional neural networks ({CNN}), a state of the art 2D feature extraction model that exploits local structures, with log power spectrogram input for {AED}. An experimental evaluation shows that the approaches we describe outperform our state-of-the-art deep learning baseline with a noticeable gain in the {CNN} case and provides insights regarding {CNN}-based spectrogram characterization for {AED}.},
  pages        = {26},
  number       = {1},
  journaltitle = {{EURASIP} Journal on Audio, Speech, and Music Processing},
  shortjournal = {J {AUDIO} {SPEECH} {MUSIC} {PROC}.},
  author       = {Espi, Miquel and Fujimoto, Masakiyo and Kinoshita, Keisuke and Nakatani, Tomohiro},
  urldate      = {2024-06-19},
  date         = {2015-12},
  langid       = {english},
  file         = {Espi et al. - 2015 - Exploiting spectro-temporal locality in deep learn.pdf:/Users/pawel/Zotero/storage/29QSQYTI/Espi et al. - 2015 - Exploiting spectro-temporal locality in deep learn.pdf:application/pdf}
}


@inproceedings{han_convolutional_2017,
  title     = {Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification},
  url       = {https://api.semanticscholar.org/CorpusID:52830611},
  booktitle = {Workshop on Detection and Classification of Acoustic Scenes and Events},
  author    = {Han, Yoonchang and Park, Jeongsoon and Lee, Kyogu},
  date      = {2017}
}


% CNN in audio no spectrograms

@inproceedings{abdel-hamid_applying_2012,
  location   = {Kyoto, Japan},
  title      = {Applying Convolutional Neural Networks concepts to hybrid {NN}-{HMM} model for speech recognition},
  isbn       = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
  url        = {http://ieeexplore.ieee.org/document/6288864/},
  doi        = {10.1109/ICASSP.2012.6288864},
  eventtitle = {{ICASSP} 2012 - 2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
  pages      = {4277--4280},
  booktitle  = {2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  publisher  = {{IEEE}},
  author     = {Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Penn, Gerald},
  urldate    = {2024-06-19},
  date       = {2012-03},
  file       = {Submitted Version:/Users/pawel/Zotero/storage/Z5P9C3WR/Abdel-Hamid et al. - 2012 - Applying Convolutional Neural Networks concepts to.pdf:application/pdf}
}

@inproceedings{sainath_deep_2013,
  location   = {Vancouver, {BC}, Canada},
  title      = {Deep convolutional neural networks for {LVCSR}},
  isbn       = {978-1-4799-0356-6},
  url        = {http://ieeexplore.ieee.org/document/6639347/},
  doi        = {10.1109/ICASSP.2013.6639347},
  abstract   = {Convolutional Neural Networks ({CNNs}) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, {CNNs} are a more effective model for speech compared to Deep Neural Networks ({DNNs}). In this paper, we explore applying {CNNs} to large vocabulary speech tasks. First, we determine the appropriate architecture to make {CNNs} effective compared to {DNNs} for {LVCSR} tasks. Speciﬁcally, we focus on how many convolutional layers are needed, what is the optimal number of hidden units, what is the best pooling strategy, and the best input feature type for {CNNs}. We then explore the behavior of neural network features extracted from {CNNs} on a variety of {LVCSR} tasks, comparing {CNNs} to {DNNs} and {GMMs}. We ﬁnd that {CNNs} offer between a 13-30\% relative improvement over {GMMs}, and a 4-12\% relative improvement over {DNNs}, on a 400-hr Broadcast News and 300-hr Switchboard task.},
  eventtitle = {{ICASSP} 2013 - 2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  pages      = {8614--8618},
  booktitle  = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
  publisher  = {{IEEE}},
  author     = {Sainath, Tara N. and Mohamed, Abdel-rahman and Kingsbury, Brian and Ramabhadran, Bhuvana},
  urldate    = {2024-06-19},
  date       = {2013-05},
  langid     = {english},
  file       = {Sainath et al. - 2013 - Deep convolutional neural networks for LVCSR.pdf:/Users/pawel/Zotero/storage/VPCY5FCK/Sainath et al. - 2013 - Deep convolutional neural networks for LVCSR.pdf:application/pdf}
}


% Recommendations

@article{noauthor_itu-r_2023,
  location = {Geneva, Switzerland},
  title    = {{ITU-R} {BS}.1770-5},
  subtitle = {Algorithms to measure audio programme loudness and true-peak audio level},
  date     = {2023-11},
  langid   = {english},
  file     = {Recommendation ITU-R BS.1770-5 (112023) Algorithm.pdf:/home/pawel/Zotero/storage/KP326E9A/Recommendation ITU-R BS.1770-5 (112023) Algorithm.pdf:application/pdf}
}


@book{kuhn_applied_2013,
  location  = {New York, {NY}},
  title     = {Applied Predictive Modeling},
  rights    = {http://www.springer.com/tdm},
  isbn      = {978-1-4614-6848-6 978-1-4614-6849-3},
  url       = {http://link.springer.com/10.1007/978-1-4614-6849-3},
  publisher = {Springer New York},
  author    = {Kuhn, Max and Johnson, Kjell},
  urldate   = {2024-06-20},
  date      = {2013},
  langid    = {english},
  doi       = {10.1007/978-1-4614-6849-3},
  file      = {Kuhn and Johnson - 2013 - Applied Predictive Modeling.pdf:/Users/pawel/Zotero/storage/5S7BX9K4/Kuhn and Johnson - 2013 - Applied Predictive Modeling.pdf:application/pdf}
}


% Automatic CNN topology selection - evaluation algorithms

@inproceedings{branke_evolutionary_1995,
  title     = {Evolutionary Algorithms for Neural Network Design and Training},
  url       = {https://www.semanticscholar.org/paper/Evolutionary-Algorithms-for-Neural-Network-Design-Branke/af9612b51f0bcab7013b239c333d17cf398d20b8},
  abstract  = {Neural networks and genetic algorithms are two relatively young research areas that were subject to a steadily growing interest during the past years. Both models are inspired by nature, but whereas neural networks are concerned with learning of an individual (phenotypic learning), evolutionary algorithms deal with a population’s adaptation to a changing environment (genotypic learning). This paper focuses on the intersection of neural networks and evolutionary computation, namely on how evolutionary algorithms can be used to assist neural network design and training. The purpose of the paper is to set forth the general considerations that have to be made when designing an algorithm in this area and to give an overview on how researchers addressed these issues in the past.},
  pages     = {145--163},
  booktitle = {Proceedings of the First Nordic Workshop on Genetic Algorithms and its Application},
  author    = {Branke, Jürgen},
  urldate   = {2024-06-23},
  date      = {1995},
  file      = {Branke - 1995 - Evolutionary Algorithms for Neural Network Design .pdf:/home/pawel/Zotero/storage/H4Y8MXEH/Branke - 1995 - Evolutionary Algorithms for Neural Network Design .pdf:application/pdf}
}

@article{zhang_finding_2018,
  title        = {Finding Better Topologies for Deep Convolutional Neural Networks by Evolution},
  volume       = {abs/1809.03242},
  url          = {https://api.semanticscholar.org/CorpusID:52182383},
  journaltitle = {{ArXiv}},
  author       = {Zhang, Honglei and Kiranyaz, Serkan and Gabbouj, M.},
  date         = {2018},
  file         = {Zhang et al. - 2018 - Finding Better Topologies for Deep Convolutional N.pdf:/home/pawel/Zotero/storage/VEYS75L8/Zhang et al. - 2018 - Finding Better Topologies for Deep Convolutional N.pdf:application/pdf}
}

@article{miikkulainen_evolving_2017,
  title        = {Evolving Deep Neural Networks},
  volume       = {abs/1703.00548},
  url          = {https://api.semanticscholar.org/CorpusID:215763844},
  journaltitle = {{ArXiv}},
  author       = {Miikkulainen, Risto and Liang, Jason Zhi and Meyerson, Elliot and Rawal, Aditya and Fink, Daniel and Francon, Olivier and Raju, Bala Eshwar and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel P. and Hodjat, Babak},
  date         = {2017},
  file         = {Miikkulainen et al. - 2017 - Evolving Deep Neural Networks.pdf:/home/pawel/Zotero/storage/HMMY2GVB/Miikkulainen et al. - 2017 - Evolving Deep Neural Networks.pdf:application/pdf}
}

@article{stanley_evolving_2002,
  title        = {Evolving Neural Networks through Augmenting Topologies},
  volume       = {10},
  issn         = {1063-6560, 1530-9304},
  url          = {https://direct.mit.edu/evco/article/10/2/99-127/1123},
  doi          = {10.1162/106365602320169811},
  abstract     = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, {NeuroEvolution} of Augmenting Topologies ({NEAT}), which outperforms the best ﬁxed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efﬁciency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signiﬁcantly faster learning. {NEAT} is also an important contribution to {GAs} because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  pages        = {99--127},
  number       = {2},
  journaltitle = {Evolutionary Computation},
  shortjournal = {Evolutionary Computation},
  author       = {Stanley, Kenneth O. and Miikkulainen, Risto},
  urldate      = {2024-06-20},
  date         = {2002-06},
  langid       = {english},
  file         = {Stanley and Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topolo.pdf:/Users/pawel/Zotero/storage/TWUN8YPM/Stanley and Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topolo.pdf:application/pdf}
}

@article{shafiee_deep_2016,
  title        = {Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural Networks},
  volume       = {48},
  url          = {https://api.semanticscholar.org/CorpusID:8106771},
  pages        = {603--613},
  journaltitle = {Neural Processing Letters},
  author       = {Shafiee, Mohammad Javad and Mishra, Akshaya Kumar and Wong, Alexander},
  date         = {2016},
  file         = {Shafiee et al. - 2017 - Deep Learning with Darwin Evolutionary Synthesis .pdf:/home/pawel/Zotero/storage/8DVHJEJP/Shafiee et al. - 2017 - Deep Learning with Darwin Evolutionary Synthesis .pdf:application/pdf}
}


% Eearly stopping


@article{pocock_practical_1989,
  title        = {Practical problems in interim analyses, with particular regard to estimation},
  volume       = {10},
  rights       = {https://www.elsevier.com/tdm/userlicense/1.0/},
  issn         = {01972456},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/0197245689900597},
  doi          = {10.1016/0197-2456(89)90059-7},
  abstract     = {This article considers some of the practical problems inherent in interim analyses and stopping rules for randomized clinical trials. Topics covered include group sequential designs, trials with unplanned interim analyses, estimation problems in clinical trials with planned interim analyses, and the balance between individual and collective ethics. Particular attention is paid to the fact that clinical trials that stop early are prone to exaggerate the magnitude of treatment effect. Accordingly, a Bayesian "shrinkage" method of analysis is proposed to help quantify the extent to which surprisingly large point and interval estimates of treatment difference in clinical trials that stop early should be moderated.},
  pages        = {209--221},
  number       = {4},
  journaltitle = {Controlled Clinical Trials},
  shortjournal = {Controlled Clinical Trials},
  author       = {Pocock, Stuart J. and Hughes, Michael D.},
  urldate      = {2024-06-20},
  date         = {1989-12},
  langid       = {english},
  file         = {Pocock and Hughes - 1989 - Practical problems in interim analyses, with parti.pdf:/Users/pawel/Zotero/storage/QB3HUWB9/Pocock and Hughes - 1989 - Practical problems in interim analyses, with parti.pdf:application/pdf}
}

@inproceedings{morgan_generalization_1989,
  title      = {Generalization and Parameter Estimation in Feedforward Nets: Some Experiments},
  volume     = {2},
  url        = {https://papers.nips.cc/paper/1989/hash/63923f49e5241343aa7acb6a06a751e7-Abstract.html},
  shorttitle = {Generalization and Parameter Estimation in Feedforward Nets},
  abstract   = {We have done an empirical study of the relation of the number of  parameters (weights) in a feedforward net to generalization perfor(cid:173) mance. Two experiments are reported. In one, we use simulated data  sets with well-controlled parameters, such as the signal-to-noise ratio  of continuous-valued data. In the second, we train the network on  vector-quantized mel cepstra from real speech samples. In each case,  we use back-propagation to train the feedforward net to discriminate in  a multiple class pattern classification problem. We report the results of  these studies, and show the application of cross-validation techniques  to prevent overfitting.},
  booktitle  = {Advances in Neural Information Processing Systems},
  publisher  = {Morgan-Kaufmann},
  author     = {Morgan, N. and Bourlard, H.},
  urldate    = {2024-06-20},
  date       = {1989},
  file       = {Full Text PDF:/Users/pawel/Zotero/storage/SZJLY474/Morgan and Bourlard - 1989 - Generalization and Parameter Estimation in Feedfor.pdf:application/pdf}
}

@article{eisenman_check-n-run_2020,
  title        = {Check-N-Run: A Checkpointing System for Training Recommendation Models},
  volume       = {abs/2010.08679},
  url          = {https://api.semanticscholar.org/CorpusID:224704491},
  journaltitle = {{ArXiv}},
  author       = {Eisenman, Assaf and Matam, Kiran Kumar and Ingram, Steven and Mudigere, Dheevatsa and Krishnamoorthi, Raghuraman and Annavaram, Murali and Nair, Krishnakumar and Smelyanskiy, Mikhail},
  date         = {2020},
  file         = {Eisenman et al. - Check-N-Run a Checkpointing System for Training D.pdf:/Users/pawel/Zotero/storage/LW5BQQ8I/Eisenman et al. - Check-N-Run a Checkpointing System for Training D.pdf:application/pdf}
}

@article{kingma_adam_2014,
  title        = {Adam: A Method for Stochastic Optimization},
  volume       = {abs/1412.6980},
  url          = {https://api.semanticscholar.org/CorpusID:6628106},
  journaltitle = {{CoRR}},
  author       = {Kingma, Diederik P. and Ba, Jimmy},
  date         = {2014},
  file         = {Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:/home/pawel/Zotero/storage/78ZYK5H2/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

% End to end localization

@article{vera-diaz_towards_2018,
  title        = {Towards End-to-End Acoustic Localization Using Deep Learning: From Audio Signals to Source Position Coordinates},
  volume       = {18},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {1424-8220},
  url          = {https://www.mdpi.com/1424-8220/18/10/3418},
  doi          = {10.3390/s18103418},
  shorttitle   = {Towards End-to-End Acoustic Localization Using Deep Learning},
  abstract     = {This paper presents a novel approach for indoor acoustic source localization using microphone arrays, based on a Convolutional Neural Network ({CNN}). In the proposed solution, the {CNN} is designed to directly estimate the three-dimensional position of a single acoustic source using the raw audio signal as the input information and avoiding the use of hand-crafted audio features. Given the limited amount of available localization data, we propose, in this paper, a training strategy based on two steps. We first train our network using semi-synthetic data generated from close talk speech recordings. We simulate the time delays and distortion suffered in the signal that propagate from the source to the array of microphones. We then fine tune this network using a small amount of real data. Our experimental results, evaluated on a publicly available dataset recorded in a real room, show that this approach is able to produce networks that significantly improve existing localization methods based on {SRP}-{PHAT} strategies and also those presented in very recent proposals based on Convolutional Recurrent Neural Networks ({CRNN}). In addition, our experiments show that the performance of our {CNN} method does not show a relevant dependency on the speaker’s gender, nor on the size of the signal window being used.},
  pages        = {3418},
  number       = {10},
  journaltitle = {Sensors},
  author       = {Vera-Diaz, Juan Manuel and Pizarro, Daniel and Macias-Guarasa, Javier},
  urldate      = {2024-06-07},
  date         = {2018-10},
  langid       = {english},
  note         = {Number: 10
                  Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {convolutional neural networks, deep learning, acoustic source localization, microphone arrays},
  file         = {Full Text PDF:/Users/pawel/Zotero/storage/QWRE2HZA/Vera-Diaz et al. - 2018 - Towards End-to-End Acoustic Localization Using Dee.pdf:application/pdf}
}


@article{vecchiotti_end--end_2019,
  title        = {End-to-end Binaural Sound Localisation from the Raw Waveform},
  url          = {https://api.semanticscholar.org/CorpusID:102481341},
  pages        = {451--455},
  journaltitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  author       = {Vecchiotti, Paolo and Ma, Ning and Squartini, Stefano and Brown, Guy J.},
  date         = {2019},
  file         = {arXiv Fulltext PDF:/home/pawel/Zotero/storage/73M9MTKE/Vecchiotti et al. - 2019 - End-to-end Binaural Sound Localisation from the Ra.pdf:application/pdf}
}


% software

@software{MATLAB,
  year      = {2022},
  author    = {{The MathWorks Inc.}},
  title     = {MATLAB version: 9.13.0 (R2022b)},
  publisher = {{The MathWorks Inc.}},
  address   = {Natick, Massachusetts, United States},
  url       = {https://www.mathworks.com}
}

@software{MATLAB_Audio_Toolbox,
  year      = {2022},
  author    = {{The MathWorks Inc.}},
  title     = {Audio Toolbox version: 9.13.0 (R2022b)},
  publisher = {{The MathWorks Inc.}},
  address   = {Natick, Massachusetts, United States},
  url       = {https://www.mathworks.com}
}

@book{python,
  author    = {Van Rossum, Guido and Drake, Fred L.},
  title     = {Python 3 Reference Manual},
  year      = {2009},
  isbn      = {1441412697},
  publisher = {CreateSpace},
  address   = {Scotts Valley, CA}
}

@article{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
             Haberland, Matt and Reddy, Tyler and Cournapeau, David and
             Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
             Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
             Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
             Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
             Kern, Robert and Larson, Eric and Carey, C J and
             Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
             {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
             Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
             Harris, Charles R. and Archibald, Anne M. and
             Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
             {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
             Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2}
}

@misc{chollet2015keras,
  title        = {Keras},
  author       = {Chollet, Fran\c{c}ois and others},
  year         = {2015},
  howpublished = {\url{https://keras.io}}
}

@software{github_repo,
  year   = {2024},
  author = {Antoniuk, Paweł},
  title  = {Software Repository: Predicting Ensemble Location and Width in Binaural Recordings of Music with Convolutional Neural Networks},
  url    = {https://github.com/pawel-antoniuk/ensemble-width-cnn/tree/main}
}

% HRTF Development durning life

@article{king_how_2001,
  title        = {How Plastic Is Spatial Hearing?},
  volume       = {6},
  issn         = {1420-3030},
  url          = {https://doi.org/10.1159/000046829},
  doi          = {10.1159/000046829},
  abstract     = {The location of a sound source is derived by the auditory system from spatial cues present in the signals at the two ears. These cues include interaural timing and level differences, as well as monaural spectral cues generated by the external ear. The values of these cues vary with individual differences in the shape and dimensions of the head and external ears. We have examined the neurophysiological consequences of these intersubject variations by recording the responses of neurons in ferret primary auditory cortex to virtual sound sources mimicking the animal’s own ears or those of other ferrets. For most neurons, the structure of the spatial response fields changed significantly when acoustic cues measured from another animal were presented. This is consistent with the finding that humans localize less accurately when listening to virtual sounds from other subjects. To examine the role of experience in shaping the ability to localize sound, we have studied the behavioural consequences of altering binaural cues by chronically plugging one ear. Ferrets raised and tested with one ear plugged learned to localize as accurately as control animals, which is consistent with previous findings that the representation of auditory space in the midbrain can accommodate abnormal sensory cues during development. Adaptive changes in behaviour were also observed in adults, particularly if they were provided with regular practice in the localization task. Together, these findings suggest that the neural circuits responsible for sound localization can be recalibrated throughout life.},
  pages        = {182--186},
  number       = {4},
  journaltitle = {Audiology and Neurotology},
  shortjournal = {Audiology and Neurotology},
  author       = {King, Andrew J. and Kacelnik, Oliver and Mrsic-Flogel, Thomas D. and Schnupp, Jan W.H. and Parsons, Carl H. and Moore, David R.},
  urldate      = {2024-06-24},
  date         = {2001-11-08}
}



@article{clifton_growth_1988,
	title = {Growth in head size during infancy: Implications for sound localization.},
	volume = {24},
	issn = {1939-0599(Electronic),0012-1649(Print)},
	doi = {10.1037/0012-1649.24.4.477},
	abstract = {We measured head circumference and interaural distance in infants between birth and 22 weeks of age. A small sample of preschool children and adults were measured for comparison over the life span. We used these data to calculate changing interaural time differences across ages. Large shifts in this important binaural cue suggest that an ongoing developmental process recalibrates the association between interaural time differences and spatial location. These new data confirmed the sex differences in head circumference described in the Berkeley Growth Study (Eichorn \& Bailey, 1962) and found no secular trend in this measure in the 60 years since the earlier data were collected. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {477--483},
	number = {4},
	journaltitle = {Developmental Psychology},
	author = {Clifton, Rachel K. and Gwiazda, Jane and Bauer, Joseph A. and Clarkson, Marsha G. and Held, Richard M.},
	date = {1988},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {*Auditory Localization, *Age Differences, *Head (Anatomy), *Human Sex Differences, Physical Development},
}
